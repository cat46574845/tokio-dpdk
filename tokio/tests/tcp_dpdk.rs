//! Tests for TcpDpdkStream and DPDK-specific functionality.
//!
//! These tests verify:
//! 1. TcpDpdkStream API parity with tokio::net::TcpStream
//! 2. DPDK-specific features (buffer pool, zero-copy, etc.)
//! 3. TcpDpdkListener functionality
//! 4. Split halves (ReadHalf/WriteHalf)

#![cfg(feature = "full")]
#![cfg(not(miri))]

use std::sync::Arc;
use std::time::Duration;

use tokio::io::{AsyncReadExt, AsyncWriteExt};
use tokio::runtime::Runtime;
use tokio::sync::oneshot;

/// Helper to create a DPDK runtime for testing.
/// Requires real DPDK environment - no fallback.
fn dpdk_rt() -> Arc<Runtime> {
    tokio::runtime::Builder::new_dpdk()
        .worker_threads(1)
        .enable_all()
        .build()
        .expect("DPDK runtime creation failed - ensure DPDK is properly configured and env.json exists")
        .into()
}

/// Helper to create a standard multi-thread runtime for comparison tests.
#[allow(dead_code)]
fn standard_rt() -> Arc<Runtime> {
    tokio::runtime::Builder::new_multi_thread()
        .worker_threads(1)
        .enable_all()
        .build()
        .unwrap()
        .into()
}

/// Get the primary kernel IP address from env.json for TCP server.
fn get_kernel_ip() -> String {
    const CONFIG_PATHS: &[&str] = &[
        "/etc/dpdk/env.json",
        "./config/dpdk-env.json",
        "./dpdk-env.json",
    ];

    for path in CONFIG_PATHS {
        if let Ok(content) = std::fs::read_to_string(path) {
            if let Ok(json) = serde_json::from_str::<serde_json::Value>(&content) {
                if let Some(devices) = json.get("devices").and_then(|d| d.as_array()) {
                    for device in devices {
                        if device.get("role").and_then(|r| r.as_str()) == Some("kernel") {
                            if let Some(addrs) = device.get("addresses").and_then(|a| a.as_array())
                            {
                                for addr in addrs {
                                    if let Some(addr_str) = addr.as_str() {
                                        if !addr_str.contains(':') {
                                            return addr_str.split('/').next().unwrap().to_string();
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
    panic!("No kernel IP found in env.json")
}

/// Get the primary DPDK IP address from env.json.
fn get_dpdk_ip() -> String {
    const CONFIG_PATHS: &[&str] = &[
        "/etc/dpdk/env.json",
        "./config/dpdk-env.json",
        "./dpdk-env.json",
    ];

    for path in CONFIG_PATHS {
        if let Ok(content) = std::fs::read_to_string(path) {
            if let Ok(json) = serde_json::from_str::<serde_json::Value>(&content) {
                if let Some(devices) = json.get("devices").and_then(|d| d.as_array()) {
                    for device in devices {
                        if device.get("role").and_then(|r| r.as_str()) == Some("dpdk") {
                            if let Some(addrs) = device.get("addresses").and_then(|a| a.as_array())
                            {
                                for addr in addrs {
                                    if let Some(addr_str) = addr.as_str() {
                                        if !addr_str.contains(':') {
                                            return addr_str.split('/').next().unwrap().to_string();
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
    panic!("No DPDK IP found in env.json")
}

/// Get a DPDK device PCI address from env.json.
fn get_dpdk_pci() -> String {
    const CONFIG_PATHS: &[&str] = &[
        "/etc/dpdk/env.json",
        "./config/dpdk-env.json",
        "./dpdk-env.json",
    ];

    for path in CONFIG_PATHS {
        if let Ok(content) = std::fs::read_to_string(path) {
            if let Ok(json) = serde_json::from_str::<serde_json::Value>(&content) {
                if let Some(devices) = json.get("devices").and_then(|d| d.as_array()) {
                    for device in devices {
                        if device.get("role").and_then(|r| r.as_str()) == Some("dpdk") {
                            if let Some(pci) = device.get("pci_address").and_then(|p| p.as_str()) {
                                return pci.to_string();
                            }
                        }
                    }
                }
            }
        }
    }
    panic!("No DPDK device PCI address found in env.json")
}

/// Get the test port from DPDK_TEST_PORT environment variable.
/// Panics if not set.
fn get_test_port() -> u16 {
    std::env::var("DPDK_TEST_PORT")
        .expect("DPDK_TEST_PORT environment variable is required")
        .parse()
        .expect("DPDK_TEST_PORT must be a valid port number")
}

// =============================================================================
// TcpDpdkStream API Parity Tests
// =============================================================================

mod api_parity {
    use super::*;
    use tokio::net::{TcpDpdkStream, TcpListener};

    /// Test that TcpDpdkStream can be created via connect()
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn tcp_dpdk_stream_connect() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            // Create kernel TCP listener as server
            let listener = TcpListener::bind(&addr).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            // Spawn accept task
            let accept_task = tokio::spawn(async move {
                let (stream, _) = listener.accept().await.unwrap();
                stream
            });

            // Connect using TcpDpdkStream (via DPDK stack)
            let client = TcpDpdkStream::connect(server_addr).await.unwrap();

            // Verify connection
            assert!(client.peer_addr().is_ok());
            assert!(client.local_addr().is_ok());

            let _server = accept_task.await.unwrap();
        });
    }

    /// Test AsyncRead/AsyncWrite traits
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn tcp_stream_read_write() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            let server_task = tokio::spawn(async move {
                let (mut stream, _) = listener.accept().await.unwrap();

                // Read from client
                let mut buf = [0u8; 11];
                stream.read_exact(&mut buf).await.unwrap();
                assert_eq!(&buf, b"hello world");

                // Write response
                stream.write_all(b"goodbye").await.unwrap();
            });

            let mut client = TcpDpdkStream::connect(server_addr).await.unwrap();

            // Write to server
            client.write_all(b"hello world").await.unwrap();

            // Read response
            let mut buf = [0u8; 7];
            client.read_exact(&mut buf).await.unwrap();
            assert_eq!(&buf, b"goodbye");

            server_task.await.unwrap();
        });
    }

    /// Test split() functionality
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn tcp_stream_split() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            let server_task = tokio::spawn(async move {
                let (mut stream, _) = listener.accept().await.unwrap();

                // Read and echo back
                let mut buf = [0u8; 5];
                stream.read_exact(&mut buf).await.unwrap();
                stream.write_all(&buf).await.unwrap();
            });

            let client = TcpDpdkStream::connect(server_addr).await.unwrap();
            let (mut read_half, mut write_half) = client.into_split();

            // Send data
            write_half.write_all(b"hello").await.unwrap();

            // Read echo
            let mut buf = [0u8; 5];
            read_half.read_exact(&mut buf).await.unwrap();
            assert_eq!(&buf, b"hello");

            server_task.await.unwrap();
        });
    }

    /// Test peek() functionality
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn tcp_stream_peek() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            let server_task = tokio::spawn(async move {
                let (mut stream, _) = listener.accept().await.unwrap();
                stream.write_all(b"peek test").await.unwrap();
                // Keep connection alive for peek
                tokio::time::sleep(Duration::from_secs(1)).await;
            });

            let client = TcpDpdkStream::connect(server_addr).await.unwrap();

            // Wait for data
            client.readable().await.unwrap();

            // Peek should not consume data
            let mut buf = [0u8; 9];
            let n = client.peek(&mut buf).await.unwrap();
            assert!(n > 0);

            server_task.abort();
        });
    }

    /// Test nodelay option
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn tcp_stream_nodelay() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            let accept_task = tokio::spawn(async move {
                let _ = listener.accept().await;
            });

            let client = TcpDpdkStream::connect(server_addr).await.unwrap();

            // Set and get nodelay
            client.set_nodelay(true).unwrap();
            assert!(client.nodelay().unwrap());

            client.set_nodelay(false).unwrap();
            assert!(!client.nodelay().unwrap());

            accept_task.abort();
        });
    }

    /// Test readable/writable async methods
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn tcp_stream_readable_writable() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            let server_task = tokio::spawn(async move {
                let (mut stream, _) = listener.accept().await.unwrap();
                tokio::time::sleep(Duration::from_millis(50)).await;
                stream.write_all(b"data").await.unwrap();
                // Keep alive
                tokio::time::sleep(Duration::from_secs(1)).await;
            });

            let client = TcpDpdkStream::connect(server_addr).await.unwrap();

            // Should be writable immediately
            client.writable().await.unwrap();

            // Wait for readable
            client.readable().await.unwrap();

            server_task.abort();
        });
    }
}

// =============================================================================
// TcpDpdkListener Tests
// =============================================================================

mod listener_tests {
    use super::*;
    use tokio::net::{TcpDpdkListener, TcpStream};

    /// Test basic accept functionality
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn listener_accept() {
        let rt = dpdk_rt();
        let dpdk_ip = get_dpdk_ip();
        let port = get_test_port();
        let addr_str = format!("{}:{}", dpdk_ip, port);

        rt.block_on(async {
            let listener = TcpDpdkListener::bind(&addr_str).await.unwrap();
            let listen_addr = listener.local_addr().unwrap();

            // Client connects via kernel TCP (routed through VPC)
            let client_task =
                tokio::spawn(async move { TcpStream::connect(listen_addr).await.unwrap() });

            let (stream, peer_addr) = listener.accept().await.unwrap();
            assert!(peer_addr.port() > 0);
            assert!(stream.peer_addr().is_ok());

            client_task.await.unwrap();
        });
    }

    /// Test multiple accepts
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn listener_multiple_accepts() {
        let rt = dpdk_rt();
        let dpdk_ip = get_dpdk_ip();
        let port = get_test_port();
        let addr_str = format!("{}:{}", dpdk_ip, port);

        rt.block_on(async {
            let listener = TcpDpdkListener::bind(&addr_str).await.unwrap();
            let listen_addr = listener.local_addr().unwrap();

            const NUM_CLIENTS: usize = 5;

            // Spawn clients using kernel TCP
            for _ in 0..NUM_CLIENTS {
                let addr = listen_addr;
                tokio::spawn(async move {
                    let _ = TcpStream::connect(addr).await;
                    tokio::time::sleep(Duration::from_millis(100)).await;
                });
            }

            // Accept all
            for _ in 0..NUM_CLIENTS {
                let result = tokio::time::timeout(Duration::from_secs(5), listener.accept()).await;
                assert!(result.is_ok());
            }
        });
    }

    /// Test local_addr
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn listener_local_addr() {
        let rt = dpdk_rt();
        let dpdk_ip = get_dpdk_ip();
        let port = get_test_port();
        let addr_str = format!("{}:{}", dpdk_ip, port);

        rt.block_on(async {
            let listener = TcpDpdkListener::bind(&addr_str).await.unwrap();
            let addr = listener.local_addr().unwrap();

            assert_eq!(addr.ip().to_string(), dpdk_ip);
            assert_eq!(addr.port(), port);
        });
    }
}

// =============================================================================
// DPDK-Specific Feature Tests
// =============================================================================

mod dpdk_specific {
    use super::*;

    /// Test DPDK runtime builder configuration
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn dpdk_builder_configuration() {
        // Test that Builder::new_dpdk() creates a valid builder
        let mut builder = tokio::runtime::Builder::new_dpdk();

        // Test device configuration (this won't actually work without DPDK)
        // These methods return &mut Builder, so we use them in-place
        builder.dpdk_pci_addresses(&[&get_dpdk_pci()]);
        builder.dpdk_mempool_size(16384);
        builder.dpdk_cache_size(512);
        builder.dpdk_queue_descriptors(256);

        // Building will fail without DPDK, but configuration should work
        let result = builder.build();
        // Expected to fail on systems without DPDK
        assert!(result.is_err() || result.is_ok());
    }

    /// Test runtime spawn and basic task execution
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn dpdk_runtime_spawn() {
        let rt = dpdk_rt();

        let result = rt.block_on(async {
            let (tx, rx) = oneshot::channel();

            tokio::spawn(async move {
                tx.send(42).unwrap();
            });

            rx.await.unwrap()
        });

        assert_eq!(result, 42);
    }

    /// Test timer functionality in DPDK runtime
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn dpdk_runtime_timer() {
        let rt = dpdk_rt();

        rt.block_on(async {
            let start = std::time::Instant::now();
            tokio::time::sleep(Duration::from_millis(50)).await;
            let elapsed = start.elapsed();

            assert!(elapsed >= Duration::from_millis(45));
            assert!(elapsed < Duration::from_secs(1));
        });
    }

    /// Test spawn_blocking in DPDK runtime
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn dpdk_runtime_spawn_blocking() {
        let rt = dpdk_rt();

        let result = rt.block_on(async {
            tokio::task::spawn_blocking(|| {
                std::thread::sleep(Duration::from_millis(10));
                "blocking result"
            })
            .await
            .unwrap()
        });

        assert_eq!(result, "blocking result");
    }

    /// Test multiple concurrent tasks
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn dpdk_runtime_concurrent_tasks() {
        let rt = dpdk_rt();

        rt.block_on(async {
            let mut handles = vec![];

            for i in 0..10 {
                handles.push(tokio::spawn(async move {
                    tokio::time::sleep(Duration::from_millis(10)).await;
                    i
                }));
            }

            let mut results = vec![];
            for handle in handles {
                results.push(handle.await.unwrap());
            }

            results.sort();
            assert_eq!(results, (0..10).collect::<Vec<_>>());
        });
    }

    /// Test channel communication across tasks
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn dpdk_runtime_channels() {
        let rt = dpdk_rt();

        rt.block_on(async {
            use tokio::sync::mpsc;

            let (tx, mut rx) = mpsc::channel(10);

            tokio::spawn(async move {
                for i in 0..5 {
                    tx.send(i).await.unwrap();
                }
            });

            let mut received = vec![];
            while let Some(val) = rx.recv().await {
                received.push(val);
            }

            assert_eq!(received, vec![0, 1, 2, 3, 4]);
        });
    }
}

// =============================================================================
// Error Handling Tests
// =============================================================================

mod error_handling {
    use super::*;
    use tokio::net::TcpDpdkStream;

    /// Test connection refused
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn connection_refused() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();

        rt.block_on(async {
            // Try to connect to a port that's not listening (port 1 is privileged)
            let addr = format!("{}:1", kernel_ip);
            let result = TcpDpdkStream::connect(&addr).await;
            assert!(result.is_err());
        });
    }

    /// Test connection timeout (if supported)
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn connection_timeout() {
        let rt = dpdk_rt();

        rt.block_on(async {
            // Try to connect with timeout
            let result = tokio::time::timeout(
                Duration::from_millis(100),
                TcpDpdkStream::connect("10.255.255.1:80"), // Non-routable address
            )
            .await;

            // Should timeout
            assert!(result.is_err());
        });
    }
}

// =============================================================================
// Shutdown Tests
// =============================================================================

mod shutdown_tests {
    use super::*;
    use tokio::net::TcpDpdkListener;

    /// Test graceful runtime shutdown
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn graceful_shutdown() {
        let rt = dpdk_rt();
        let dpdk_ip = get_dpdk_ip();
        let port = get_test_port();
        let addr_str = format!("{}:{}", dpdk_ip, port);

        rt.block_on(async {
            let listener = TcpDpdkListener::bind(&addr_str).await.unwrap();
            let _addr = listener.local_addr().unwrap();

            // Spawn a task that will be cancelled on shutdown
            tokio::spawn(async {
                loop {
                    tokio::time::sleep(Duration::from_secs(1)).await;
                }
            });
        });

        // Runtime should shut down cleanly when dropped
        drop(rt);
    }
}

// NOTE: The DPDK runtime does not support #[tokio::test] / #[tokio::main] macros.
// Use Builder::new_dpdk() + #[serial_isolation_test] + #[test] + rt.block_on() instead.

// =============================================================================
// Stream/Listener Property Tests (core_id, peek verification, etc.)
// =============================================================================

mod stream_property_tests {
    use super::*;
    use tokio::net::{TcpDpdkListener, TcpDpdkStream, TcpListener};

    /// Test that core_id() returns a valid value
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn stream_core_id_valid() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            let accept_task = tokio::spawn(async move {
                let (stream, _) = listener.accept().await.unwrap();
                stream
            });

            let client = TcpDpdkStream::connect(server_addr).await.unwrap();

            // Verify core_id is accessible
            let core_id = client.core_id();
            assert!(core_id < 64, "core_id should be reasonable");

            let _server = accept_task.await.unwrap();
        });
    }

    /// Test peek() with data verification
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn stream_peek_data_integrity() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            let server_task = tokio::spawn(async move {
                let (mut stream, _) = listener.accept().await.unwrap();
                stream.write_all(b"PEEK_TEST_DATA").await.unwrap();
                // Keep connection open
                tokio::time::sleep(Duration::from_millis(500)).await;
            });

            let client = TcpDpdkStream::connect(server_addr).await.unwrap();

            // Wait for data to arrive
            client.readable().await.unwrap();

            // Peek should not consume data
            let mut peek_buf = [0u8; 14];
            let peek_n = client.peek(&mut peek_buf).await.unwrap();
            assert!(peek_n > 0, "peek should return some data");

            // Now read the same data - it should still be there
            let mut read_buf = [0u8; 14];
            let mut total_read = 0;
            while total_read < 14 {
                client.readable().await.unwrap();
                match client.try_read(&mut read_buf[total_read..]) {
                    Ok(n) if n > 0 => total_read += n,
                    Ok(_) => break,
                    Err(ref e) if e.kind() == std::io::ErrorKind::WouldBlock => continue,
                    Err(e) => panic!("read error: {}", e),
                }
            }

            // Verify peek and read got the same data
            assert_eq!(&peek_buf[..peek_n], &read_buf[..peek_n]);
            assert_eq!(&read_buf[..total_read], b"PEEK_TEST_DATA");

            server_task.abort();
        });
    }

    /// Test that split halves can be used independently
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn stream_split_independent_use() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            let server_task = tokio::spawn(async move {
                let (mut stream, _) = listener.accept().await.unwrap();
                // Server reads then writes
                let mut buf = [0u8; 11];
                stream.read_exact(&mut buf).await.unwrap();
                assert_eq!(&buf, b"from client");
                stream.write_all(b"from server").await.unwrap();
            });

            let client = TcpDpdkStream::connect(server_addr).await.unwrap();
            let (mut read_half, mut write_half) = client.into_split();

            // Write first
            write_half.write_all(b"from client").await.unwrap();

            // Then read
            let mut buf = [0u8; 11];
            read_half.read_exact(&mut buf).await.unwrap();
            assert_eq!(&buf, b"from server");

            server_task.await.unwrap();
        });
    }

    /// Test listener core_id returns valid value
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn listener_core_id_valid() {
        let rt = dpdk_rt();
        let dpdk_ip = get_dpdk_ip();
        let port = get_test_port();
        let addr_str = format!("{}:{}", dpdk_ip, port);

        rt.block_on(async {
            let listener = TcpDpdkListener::bind(&addr_str).await.unwrap();
            let core_id = listener.core_id();
            assert!(core_id < 64, "core_id should be reasonable");
            let addr = listener.local_addr().unwrap();
            assert_eq!(addr.port(), port);
        });
    }
}

// =============================================================================
// Worker Affinity Tests
// =============================================================================

/// Tests for worker affinity enforcement.
/// TcpDpdkStream must be used on the same worker where it was created.
#[cfg(all(target_os = "linux", feature = "full"))]
mod worker_affinity_tests {
    use super::*;
    use std::sync::atomic::{AtomicBool, AtomicUsize, Ordering};
    use std::sync::Arc as StdArc;

    /// Test that TcpDpdkStream enforces worker affinity on read operations.
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn stream_worker_affinity_enforcement() {
        // Create runtime with 2 workers
        let rt = tokio::runtime::Builder::new_dpdk()
            .worker_threads(2)
            .enable_all()
            .build()
            .expect("Failed to create multi-worker DPDK runtime");

        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr_str = format!("{}:{}", kernel_ip, port);

        let affinity_checked = StdArc::new(AtomicBool::new(false));
        let affinity_checked_clone = affinity_checked.clone();

        rt.block_on(async {
            use tokio::net::{TcpDpdkStream, TcpListener};

            let workers = tokio::runtime::dpdk::workers();
            assert!(
                workers.len() >= 2,
                "Multi-worker test requires at least 2 workers, got {}",
                workers.len()
            );

            let listener = TcpListener::bind(&addr_str).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            let server_task = tokio::spawn(async move {
                let (stream, _) = listener.accept().await.unwrap();
                drop(stream);
            });

            let client = TcpDpdkStream::connect(server_addr).await.unwrap();

            // Verify the stream works correctly
            assert!(client.local_addr().is_ok());
            assert!(client.peer_addr().is_ok());
            let core_id = client.core_id();
            assert!(core_id < 64, "core_id should be reasonable");

            affinity_checked_clone.store(true, Ordering::SeqCst);

            drop(client);
            server_task.await.ok();
        });

        assert!(affinity_checked.load(Ordering::SeqCst));
    }

    /// Test that creating streams on different workers works independently
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn streams_on_different_workers() {
        let rt = tokio::runtime::Builder::new_dpdk()
            .worker_threads(2)
            .enable_all()
            .build()
            .expect("Failed to create multi-worker DPDK runtime");

        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr_str = format!("{}:{}", kernel_ip, port);

        let streams_created = StdArc::new(AtomicUsize::new(0));

        rt.block_on(async {
            use tokio::net::{TcpDpdkStream, TcpListener};

            let workers = tokio::runtime::dpdk::workers();
            assert!(
                workers.len() >= 2,
                "Multi-worker test requires at least 2 workers, got {}",
                workers.len()
            );

            let listener = TcpListener::bind(&addr_str).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            // Spawn multiple tasks that may run on different workers
            let mut handles = vec![];

            for _ in 0..4 {
                let addr = server_addr;
                let streams_created = streams_created.clone();

                handles.push(tokio::spawn(async move {
                    if let Ok(stream) = TcpDpdkStream::connect(addr).await {
                        assert!(stream.local_addr().is_ok());
                        streams_created.fetch_add(1, Ordering::SeqCst);
                    }
                }));
            }

            // Accept connections
            let accept_task = tokio::spawn(async move {
                let mut accepted = 0;
                for _ in 0..4 {
                    match tokio::time::timeout(Duration::from_secs(2), listener.accept()).await {
                        Ok(Ok(_)) => accepted += 1,
                        _ => break,
                    }
                }
                accepted
            });

            for handle in handles {
                handle.await.ok();
            }

            let accepted = accept_task.await.unwrap();
            assert!(accepted > 0, "At least one connection should be accepted");
        });

        assert!(streams_created.load(Ordering::SeqCst) > 0);
    }

    /// Test that worker affinity warning is printed on Drop from wrong worker
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn drop_on_wrong_worker_warns_not_panics() {
        let rt = tokio::runtime::Builder::new_dpdk()
            .worker_threads(2)
            .enable_all()
            .build()
            .expect("Failed to create multi-worker DPDK runtime");

        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr_str = format!("{}:{}", kernel_ip, port);

        let completed = StdArc::new(AtomicBool::new(false));
        let completed_clone = completed.clone();

        rt.block_on(async {
            use tokio::net::{TcpDpdkStream, TcpListener};

            let workers = tokio::runtime::dpdk::workers();
            assert!(
                workers.len() >= 2,
                "Multi-worker test requires at least 2 workers, got {}",
                workers.len()
            );

            let listener = TcpListener::bind(&addr_str).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            let accept_task = tokio::spawn(async move {
                tokio::time::timeout(Duration::from_secs(1), listener.accept())
                    .await
                    .ok();
            });

            let client = TcpDpdkStream::connect(server_addr).await.ok();
            drop(client); // Should not panic

            accept_task.await.ok();
            completed_clone.store(true, Ordering::SeqCst);
        });

        assert!(
            completed.load(Ordering::SeqCst),
            "Test should complete without panic"
        );
    }
}

// =============================================================================
// Multi-Worker Scheduling Tests
// =============================================================================

mod multi_worker_tests {
    use super::*;
    use std::sync::atomic::{AtomicUsize, Ordering};

    /// Test that tasks can be spawned and executed with multiple workers
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn multi_worker_task_distribution() {
        // Create runtime with multiple workers if supported
        let rt = tokio::runtime::Builder::new_dpdk()
            .worker_threads(2) // Request 2 workers
            .enable_all()
            .build()
            .expect("Failed to create multi-worker DPDK runtime");

        let counter = Arc::new(AtomicUsize::new(0));
        let num_tasks = 100;

        rt.block_on(async {
            let workers = tokio::runtime::dpdk::workers();
            assert!(
                workers.len() >= 2,
                "Multi-worker test requires at least 2 workers, got {}",
                workers.len()
            );

            let mut handles = vec![];

            for _ in 0..num_tasks {
                let counter = counter.clone();
                handles.push(tokio::spawn(async move {
                    // Simulate some work
                    tokio::time::sleep(Duration::from_micros(100)).await;
                    counter.fetch_add(1, Ordering::SeqCst);
                }));
            }

            // Wait for all tasks
            for handle in handles {
                handle.await.unwrap();
            }
        });

        assert_eq!(counter.load(Ordering::SeqCst), num_tasks);
    }

    /// Test that tasks spawned from different contexts complete
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn multi_worker_cross_spawn() {
        let rt = Arc::new(
            tokio::runtime::Builder::new_dpdk()
                .worker_threads(2)
                .enable_all()
                .build()
                .expect("Failed to create DPDK runtime"),
        );

        // Verify we got enough workers
        let worker_count = rt.block_on(async { tokio::runtime::dpdk::workers().len() });
        assert!(
            worker_count >= 2,
            "Multi-worker test requires at least 2 workers, got {}",
            worker_count
        );

        let rt_clone = rt.clone();
        let (tx, rx) = oneshot::channel();

        // Spawn from outside runtime
        rt.spawn(async move {
            // Spawn nested task
            let inner = tokio::spawn(async {
                tokio::time::sleep(Duration::from_millis(10)).await;
                "nested"
            });

            let result = inner.await.unwrap();
            tx.send(result).unwrap();
        });

        let result = rt_clone.block_on(async { rx.await.unwrap() });

        assert_eq!(result, "nested");
    }

    /// Test channel communication between workers
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn multi_worker_channel_communication() {
        let rt = tokio::runtime::Builder::new_dpdk()
            .worker_threads(2)
            .enable_all()
            .build()
            .expect("Failed to create DPDK runtime");

        rt.block_on(async {
            let workers = tokio::runtime::dpdk::workers();
            assert!(
                workers.len() >= 2,
                "Multi-worker test requires at least 2 workers, got {}",
                workers.len()
            );

            use tokio::sync::mpsc;

            let (tx, mut rx) = mpsc::channel(100);

            // Producer tasks
            for i in 0..10 {
                let tx = tx.clone();
                tokio::spawn(async move {
                    for j in 0..10 {
                        tx.send(i * 10 + j).await.unwrap();
                    }
                });
            }

            drop(tx); // Close sender

            // Consumer
            let mut received = vec![];
            while let Some(val) = rx.recv().await {
                received.push(val);
            }

            assert_eq!(received.len(), 100);
        });
    }
}

// =============================================================================
// Buffer Pool and Resource Management Tests
// =============================================================================

mod buffer_pool_tests {
    use super::*;
    use tokio::net::{TcpDpdkStream, TcpListener};

    /// Test that many connections don't exhaust resources
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn many_sequential_connections() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr_str = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr_str).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            // Create and close many connections sequentially
            for i in 0..20 {
                let accept_task = tokio::spawn({
                    async move {
                        tokio::time::sleep(Duration::from_millis(1)).await;
                    }
                });

                let client = TcpDpdkStream::connect(server_addr).await;
                if client.is_err() {
                    tokio::time::sleep(Duration::from_millis(10)).await;
                    continue;
                }
                let mut client = client.unwrap();

                client.write_all(&[i as u8]).await.ok();

                let accept_result =
                    tokio::time::timeout(Duration::from_millis(100), listener.accept()).await;

                if accept_result.is_ok() {
                    let (mut server, _) = accept_result.unwrap().unwrap();
                    let mut buf = [0u8; 1];
                    server.read_exact(&mut buf).await.ok();
                }

                drop(client);
                accept_task.await.ok();
            }
        });
    }

    /// Test concurrent connections stress test
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn concurrent_connections_stress() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr_str = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr_str).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            const NUM_CONNECTIONS: usize = 10;

            let accept_handle = tokio::spawn(async move {
                let mut accepted = 0;
                while accepted < NUM_CONNECTIONS {
                    match tokio::time::timeout(Duration::from_secs(5), listener.accept()).await {
                        Ok(Ok((mut stream, _))) => {
                            tokio::spawn(async move {
                                let mut buf = [0u8; 64];
                                if let Ok(n) = stream.read(&mut buf).await {
                                    if n > 0 {
                                        stream.write_all(&buf[..n]).await.ok();
                                    }
                                }
                            });
                            accepted += 1;
                        }
                        Ok(Err(_)) => break,
                        Err(_) => break,
                    }
                }
                accepted
            });

            let mut client_handles = vec![];
            for i in 0..NUM_CONNECTIONS {
                let addr = server_addr;
                client_handles.push(tokio::spawn(async move {
                    let stream = TcpDpdkStream::connect(addr).await;
                    if let Ok(mut stream) = stream {
                        let msg = format!("msg{}", i);
                        stream.write_all(msg.as_bytes()).await.ok();
                        let mut buf = [0u8; 64];
                        stream.read(&mut buf).await.ok();
                        true
                    } else {
                        false
                    }
                }));
            }

            let mut successes = 0;
            for handle in client_handles {
                if handle.await.unwrap_or(false) {
                    successes += 1;
                }
            }

            let accepted = accept_handle.await.unwrap();

            assert!(
                successes >= NUM_CONNECTIONS / 2,
                "Only {} of {} connections succeeded",
                successes,
                NUM_CONNECTIONS
            );
            assert!(
                accepted >= NUM_CONNECTIONS / 2,
                "Only accepted {} of {} connections",
                accepted,
                NUM_CONNECTIONS
            );
        });
    }

    /// Test rapid create/destroy cycles (buffer pool churn)
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn buffer_pool_churn() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr_str = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr_str).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            for _ in 0..15 {
                let accept_task = tokio::spawn({
                    async move {
                        tokio::time::sleep(Duration::from_millis(50)).await;
                    }
                });

                let connect_result = tokio::time::timeout(
                    Duration::from_millis(50),
                    TcpDpdkStream::connect(server_addr),
                )
                .await;

                if let Ok(Ok(client)) = connect_result {
                    if let Ok(Ok((server, _))) =
                        tokio::time::timeout(Duration::from_millis(50), listener.accept()).await
                    {
                        drop(server);
                    }
                    drop(client);
                }

                accept_task.await.ok();
            }
        });
    }
}

// =============================================================================
// Local Overflow Queue Tests
// =============================================================================

mod local_overflow_tests {
    use super::*;
    use std::sync::atomic::{AtomicUsize, Ordering};

    /// Test that spawning many tasks doesn't lose any
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn spawn_many_tasks_no_loss() {
        let rt = dpdk_rt();

        const NUM_TASKS: usize = 1000;
        let counter = Arc::new(AtomicUsize::new(0));

        rt.block_on(async {
            let mut handles = vec![];

            for _ in 0..NUM_TASKS {
                let counter = counter.clone();
                handles.push(tokio::spawn(async move {
                    counter.fetch_add(1, Ordering::SeqCst);
                }));
            }

            // Wait for all
            for handle in handles {
                handle.await.unwrap();
            }
        });

        assert_eq!(counter.load(Ordering::SeqCst), NUM_TASKS);
    }

    /// Test rapid spawn/complete cycles
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn rapid_spawn_complete_cycles() {
        let rt = dpdk_rt();

        rt.block_on(async {
            for round in 0..10 {
                let mut handles = vec![];

                for i in 0..100 {
                    handles.push(tokio::spawn(async move { round * 100 + i }));
                }

                let mut results: Vec<usize> = vec![];
                for handle in handles {
                    results.push(handle.await.unwrap());
                }

                results.sort();
                let expected: Vec<usize> = (round * 100..(round + 1) * 100).collect();
                assert_eq!(results, expected);
            }
        });
    }

    /// Test nested spawns (tasks that spawn more tasks)
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn nested_spawn_chain() {
        let rt = dpdk_rt();

        rt.block_on(async {
            let result = tokio::spawn(async {
                let inner1 = tokio::spawn(async {
                    let inner2 = tokio::spawn(async {
                        let inner3 = tokio::spawn(async { "deepest" });
                        inner3.await.unwrap()
                    });
                    inner2.await.unwrap()
                });
                inner1.await.unwrap()
            })
            .await
            .unwrap();

            assert_eq!(result, "deepest");
        });
    }

    /// Test that yield_now works correctly
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn yield_now_fairness() {
        let rt = dpdk_rt();

        rt.block_on(async {
            let order = Arc::new(std::sync::Mutex::new(vec![]));

            let order1 = order.clone();
            let order2 = order.clone();

            let t1 = tokio::spawn(async move {
                for i in 0..5 {
                    order1.lock().unwrap().push(format!("t1-{}", i));
                    tokio::task::yield_now().await;
                }
            });

            let t2 = tokio::spawn(async move {
                for i in 0..5 {
                    order2.lock().unwrap().push(format!("t2-{}", i));
                    tokio::task::yield_now().await;
                }
            });

            t1.await.unwrap();
            t2.await.unwrap();

            let final_order = order.lock().unwrap();
            assert_eq!(final_order.len(), 10);

            // Verify both tasks made progress (interleaving not guaranteed but both should finish)
            let t1_count = final_order.iter().filter(|s| s.starts_with("t1")).count();
            let t2_count = final_order.iter().filter(|s| s.starts_with("t2")).count();
            assert_eq!(t1_count, 5);
            assert_eq!(t2_count, 5);
        });
    }

    /// Stress test: Massive concurrent task spawn (10,000 tasks)
    /// Tests local queue overflow handling under extreme load
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn stress_massive_spawn() {
        let rt = dpdk_rt();

        const NUM_TASKS: usize = 10_000;
        let counter = Arc::new(AtomicUsize::new(0));

        rt.block_on(async {
            let mut handles = Vec::with_capacity(NUM_TASKS);

            // Spawn all tasks as fast as possible
            for _ in 0..NUM_TASKS {
                let counter = counter.clone();
                handles.push(tokio::spawn(async move {
                    // Minimal work to maximize spawn pressure
                    counter.fetch_add(1, Ordering::Relaxed);
                }));
            }

            // Wait for all tasks to complete
            for handle in handles {
                handle.await.unwrap();
            }
        });

        assert_eq!(
            counter.load(Ordering::Relaxed),
            NUM_TASKS,
            "All {} tasks must complete",
            NUM_TASKS
        );
    }

    /// Stress test: Burst spawn with immediate yield
    /// Forces queue churn by rapidly spawning and yielding
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn stress_burst_spawn_yield() {
        let rt = dpdk_rt();

        const TASKS_PER_BURST: usize = 500;
        const NUM_BURSTS: usize = 20;
        let total_expected = TASKS_PER_BURST * NUM_BURSTS;
        let counter = Arc::new(AtomicUsize::new(0));

        rt.block_on(async {
            for burst in 0..NUM_BURSTS {
                let mut handles = Vec::with_capacity(TASKS_PER_BURST);

                for _ in 0..TASKS_PER_BURST {
                    let counter = counter.clone();
                    handles.push(tokio::spawn(async move {
                        // Yield immediately to stress the scheduler
                        tokio::task::yield_now().await;
                        counter.fetch_add(1, Ordering::Relaxed);
                    }));
                }

                // Wait for this burst to complete before next
                for handle in handles {
                    handle.await.unwrap();
                }

                // Verify progress after each burst
                let current = counter.load(Ordering::Relaxed);
                assert_eq!(
                    current,
                    (burst + 1) * TASKS_PER_BURST,
                    "Burst {} should complete {} tasks, got {}",
                    burst,
                    (burst + 1) * TASKS_PER_BURST,
                    current
                );
            }
        });

        assert_eq!(counter.load(Ordering::Relaxed), total_expected);
    }

    /// Stress test: Deep nested spawn chain
    /// Tests stack and queue limits with deeply nested task spawns
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn stress_deep_nested_spawn() {
        let rt = dpdk_rt();

        const DEPTH: usize = 50;
        let completed = Arc::new(AtomicUsize::new(0));

        rt.block_on(async {
            fn spawn_nested(
                depth: usize,
                completed: Arc<AtomicUsize>,
            ) -> tokio::task::JoinHandle<usize> {
                tokio::spawn(async move {
                    if depth == 0 {
                        completed.fetch_add(1, Ordering::Relaxed);
                        0
                    } else {
                        let inner = spawn_nested(depth - 1, completed.clone());
                        let result = inner.await.unwrap();
                        completed.fetch_add(1, Ordering::Relaxed);
                        result + 1
                    }
                })
            }

            let handle = spawn_nested(DEPTH, completed.clone());
            let result = handle.await.unwrap();

            assert_eq!(result, DEPTH, "Nested chain should reach depth {}", DEPTH);
        });

        // Each level increments on return, so total = DEPTH + 1
        assert_eq!(
            completed.load(Ordering::Relaxed),
            DEPTH + 1,
            "All {} nested tasks must complete",
            DEPTH + 1
        );
    }

    /// Stress test: Producer-consumer with high throughput
    /// Tests channel + spawn interaction under pressure
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn stress_producer_consumer_high_throughput() {
        let rt = dpdk_rt();

        const NUM_PRODUCERS: usize = 10;
        const MESSAGES_PER_PRODUCER: usize = 1000;
        const TOTAL_MESSAGES: usize = NUM_PRODUCERS * MESSAGES_PER_PRODUCER;

        rt.block_on(async {
            use tokio::sync::mpsc;

            let (tx, mut rx) = mpsc::channel::<usize>(1024);

            // Spawn producers
            for producer_id in 0..NUM_PRODUCERS {
                let tx = tx.clone();
                tokio::spawn(async move {
                    for i in 0..MESSAGES_PER_PRODUCER {
                        tx.send(producer_id * MESSAGES_PER_PRODUCER + i)
                            .await
                            .unwrap();
                    }
                });
            }
            drop(tx); // Close sender side

            // Consumer - collect all messages
            let mut received = Vec::with_capacity(TOTAL_MESSAGES);
            while let Some(msg) = rx.recv().await {
                received.push(msg);
            }

            assert_eq!(
                received.len(),
                TOTAL_MESSAGES,
                "Must receive all {} messages",
                TOTAL_MESSAGES
            );

            // Verify no duplicates
            let mut sorted = received.clone();
            sorted.sort();
            sorted.dedup();
            assert_eq!(
                sorted.len(),
                TOTAL_MESSAGES,
                "No duplicate messages allowed"
            );
        });
    }

    /// Stress test: Contended spawning from multiple tasks
    /// Multiple tasks simultaneously spawn new tasks to stress the global queue
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn stress_contended_spawn() {
        let rt = dpdk_rt();

        const NUM_SPAWNERS: usize = 20;
        const SPAWNS_PER_SPAWNER: usize = 100;
        let counter = Arc::new(AtomicUsize::new(0));

        rt.block_on(async {
            let barrier = Arc::new(tokio::sync::Barrier::new(NUM_SPAWNERS));
            let mut spawner_handles = Vec::with_capacity(NUM_SPAWNERS);

            for _ in 0..NUM_SPAWNERS {
                let barrier = barrier.clone();
                let counter = counter.clone();
                spawner_handles.push(tokio::spawn(async move {
                    // Wait for all spawners to be ready
                    barrier.wait().await;

                    // Now all spawn simultaneously
                    let mut handles = Vec::with_capacity(SPAWNS_PER_SPAWNER);
                    for _ in 0..SPAWNS_PER_SPAWNER {
                        let counter = counter.clone();
                        handles.push(tokio::spawn(async move {
                            counter.fetch_add(1, Ordering::Relaxed);
                        }));
                    }

                    // Wait for all spawned tasks
                    for handle in handles {
                        handle.await.unwrap();
                    }
                }));
            }

            // Wait for all spawners
            for handle in spawner_handles {
                handle.await.unwrap();
            }
        });

        let total = NUM_SPAWNERS * SPAWNS_PER_SPAWNER;
        assert_eq!(
            counter.load(Ordering::Relaxed),
            total,
            "All {} tasks must complete",
            total
        );
    }
}

// =============================================================================
// Timer Integration Tests
// =============================================================================

mod timer_tests {
    use super::*;

    /// Test multiple concurrent timers
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn concurrent_timers() {
        let rt = dpdk_rt();

        rt.block_on(async {
            let t1 = tokio::time::sleep(Duration::from_millis(50));
            let t2 = tokio::time::sleep(Duration::from_millis(100));
            let t3 = tokio::time::sleep(Duration::from_millis(150));

            let start = std::time::Instant::now();
            tokio::join!(t1, t2, t3);
            let elapsed = start.elapsed();

            // All should complete after ~150ms (the longest)
            assert!(elapsed >= Duration::from_millis(140));
            assert!(elapsed < Duration::from_millis(300));
        });
    }

    /// Test interval timer
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn interval_timer() {
        let rt = dpdk_rt();

        rt.block_on(async {
            let mut interval = tokio::time::interval(Duration::from_millis(20));
            let mut ticks = 0;

            let start = std::time::Instant::now();
            while ticks < 5 {
                interval.tick().await;
                ticks += 1;
            }
            let elapsed = start.elapsed();

            // 5 ticks at 20ms each = ~100ms (first tick is immediate)
            assert!(elapsed >= Duration::from_millis(60));
            assert!(elapsed < Duration::from_millis(200));
        });
    }

    /// Test timeout that expires
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn timeout_expires() {
        let rt = dpdk_rt();

        rt.block_on(async {
            let result = tokio::time::timeout(Duration::from_millis(50), async {
                tokio::time::sleep(Duration::from_secs(10)).await;
                "should not reach"
            })
            .await;

            assert!(result.is_err());
        });
    }

    /// Test timeout that completes in time
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn timeout_completes() {
        let rt = dpdk_rt();

        rt.block_on(async {
            let result = tokio::time::timeout(Duration::from_secs(1), async {
                tokio::time::sleep(Duration::from_millis(10)).await;
                "completed"
            })
            .await;

            assert_eq!(result.unwrap(), "completed");
        });
    }
}

// =============================================================================
// Edge Case and Error Handling Tests
// =============================================================================

mod edge_case_tests {
    use super::*;
    use tokio::net::{TcpDpdkListener, TcpDpdkStream, TcpListener};

    /// Test binding to address that's already in use
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn bind_address_in_use() {
        let rt = dpdk_rt();
        let dpdk_ip = get_dpdk_ip();
        let port = get_test_port();
        let addr_str = format!("{}:{}", dpdk_ip, port);

        rt.block_on(async {
            let listener1 = TcpDpdkListener::bind(&addr_str).await.unwrap();
            let addr = listener1.local_addr().unwrap();

            // Try to bind to the same address
            let result = TcpDpdkListener::bind(addr).await;
            assert!(result.is_err());
        });
    }

    /// Test zero-length write
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn zero_length_operations() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr_str = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr_str).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            let server_task = tokio::spawn(async move {
                let (mut stream, _) = listener.accept().await.unwrap();
                let mut buf = [0u8; 10];
                let n = stream.read(&mut buf).await.unwrap();
                assert!(n > 0);
            });

            let mut client = TcpDpdkStream::connect(server_addr).await.unwrap();

            // Zero-length write should succeed
            let n = client.write(&[]).await.unwrap();
            assert_eq!(n, 0);

            // Normal write
            client.write_all(b"hello").await.unwrap();

            server_task.await.unwrap();
        });
    }

    /// Test large data transfer
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn large_data_transfer() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr_str = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr_str).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            const DATA_SIZE: usize = 64 * 1024; // 64KB
            let data: Vec<u8> = (0..DATA_SIZE).map(|i| (i % 256) as u8).collect();

            let server_task = tokio::spawn({
                let expected_data = data.clone();
                async move {
                    let (mut stream, _) = listener.accept().await.unwrap();
                    let mut received = vec![0u8; DATA_SIZE];
                    stream.read_exact(&mut received).await.unwrap();
                    assert_eq!(received, expected_data);
                }
            });

            let mut client = TcpDpdkStream::connect(server_addr).await.unwrap();
            client.write_all(&data).await.unwrap();

            server_task.await.unwrap();
        });
    }

    /// Test half-close (shutdown write but keep reading)
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn half_close() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr_str = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr_str).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            let server_task = tokio::spawn(async move {
                let (mut stream, _) = listener.accept().await.unwrap();

                // Read from client
                let mut buf = [0u8; 5];
                stream.read_exact(&mut buf).await.unwrap();
                assert_eq!(&buf, b"hello");

                // Wait for client to close write side
                let mut buf = [0u8; 10];
                let n = stream.read(&mut buf).await.unwrap();
                assert_eq!(n, 0); // EOF

                // Send response
                stream.write_all(b"goodbye").await.unwrap();
            });

            let mut client = TcpDpdkStream::connect(server_addr).await.unwrap();

            // Write and shutdown write side
            client.write_all(b"hello").await.unwrap();
            client.shutdown().await.unwrap();

            // Should still be able to read
            let mut buf = [0u8; 7];
            client.read_exact(&mut buf).await.unwrap();
            assert_eq!(&buf, b"goodbye");

            server_task.await.unwrap();
        });
    }
}

// =============================================================================
// CPU Core Affinity Verification Tests
// =============================================================================
// These tests verify that DPDK workers and blocking threads run on their
// designated CPU cores, ensuring proper isolation and NUMA-aware scheduling.
// =============================================================================

#[cfg(all(target_os = "linux", feature = "full"))]
mod cpu_affinity_tests {
    use super::*;
    use nix::sched::{sched_getaffinity, CpuSet};
    use nix::unistd::Pid;
    use std::sync::atomic::{AtomicBool, AtomicUsize, Ordering};
    use std::sync::Arc as StdArc;

    /// Test that the DPDK worker thread runs on the designated core.
    /// Uses sched_getaffinity to verify the actual CPU affinity.
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn worker_runs_on_designated_core() {
        let rt = tokio::runtime::Builder::new_dpdk()
            .enable_all()
            .build()
            .expect("Failed to create DPDK runtime");

        let correct_core = StdArc::new(AtomicBool::new(false));
        let detected_cpu = StdArc::new(AtomicUsize::new(usize::MAX));

        let correct_core_clone = correct_core.clone();
        let detected_cpu_clone = detected_cpu.clone();

        rt.block_on(async move {
            // Spawn a task to check CPU affinity from within the worker
            let handle = tokio::spawn(async move {
                // Get the CPU affinity of the current thread
                let pid = Pid::from_raw(0); // 0 = current thread
                match sched_getaffinity(pid) {
                    Ok(cpuset) => {
                        // Find which CPU(s) we're allowed to run on
                        let mut allowed_cpus: Vec<usize> = Vec::new();
                        for cpu in 0..CpuSet::count() {
                            if cpuset.is_set(cpu).unwrap_or(false) {
                                allowed_cpus.push(cpu);
                            }
                        }

                        // For DPDK worker, we expect exactly one CPU to be set
                        // (strict affinity to designated core)
                        if allowed_cpus.len() == 1 {
                            detected_cpu_clone.store(allowed_cpus[0], Ordering::SeqCst);
                            correct_core_clone.store(true, Ordering::SeqCst);
                        } else if !allowed_cpus.is_empty() {
                            // Record first allowed CPU for diagnostics
                            detected_cpu_clone.store(allowed_cpus[0], Ordering::SeqCst);
                            // Still mark as correct if at least one CPU is set
                            // (some systems may have looser affinity)
                            correct_core_clone.store(true, Ordering::SeqCst);
                        }
                    }
                    Err(e) => {
                        eprintln!("Failed to get CPU affinity: {:?}", e);
                    }
                }
            });

            handle.await.unwrap();
        });

        let cpu = detected_cpu.load(Ordering::SeqCst);
        assert!(
            correct_core.load(Ordering::SeqCst),
            "Worker thread should have CPU affinity set (detected CPU: {})",
            if cpu == usize::MAX {
                "none".to_string()
            } else {
                cpu.to_string()
            }
        );

        println!(
            " Worker thread verified running on CPU {}",
            detected_cpu.load(Ordering::SeqCst)
        );
    }

    /// Test that blocking threads do NOT run on isolated DPDK cores.
    /// This ensures blocking operations don't interfere with low-latency networking.
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn blocking_thread_not_on_dpdk_core() {
        let rt = tokio::runtime::Builder::new_dpdk()
            .enable_all()
            .build()
            .expect("Failed to create DPDK runtime");

        let worker_cpu = StdArc::new(AtomicUsize::new(usize::MAX));
        let blocking_cpu = StdArc::new(AtomicUsize::new(usize::MAX));
        let test_completed = StdArc::new(AtomicBool::new(false));

        let worker_cpu_clone = worker_cpu.clone();
        let blocking_cpu_clone = blocking_cpu.clone();
        let test_completed_clone = test_completed.clone();

        rt.block_on(async move {
            // First, get the worker's CPU
            let pid = Pid::from_raw(0);
            if let Ok(cpuset) = sched_getaffinity(pid) {
                for cpu in 0..CpuSet::count() {
                    if cpuset.is_set(cpu).unwrap_or(false) {
                        worker_cpu_clone.store(cpu, Ordering::SeqCst);
                        break;
                    }
                }
            }

            // Now spawn a blocking task and check its CPU
            let blocking_cpu_inner = blocking_cpu_clone.clone();
            let blocking_handle = tokio::task::spawn_blocking(move || {
                let pid = Pid::from_raw(0);
                if let Ok(cpuset) = sched_getaffinity(pid) {
                    for cpu in 0..CpuSet::count() {
                        if cpuset.is_set(cpu).unwrap_or(false) {
                            blocking_cpu_inner.store(cpu, Ordering::SeqCst);
                            break;
                        }
                    }
                }
            });

            blocking_handle.await.unwrap();
            test_completed_clone.store(true, Ordering::SeqCst);
        });

        assert!(
            test_completed.load(Ordering::SeqCst),
            "Test should complete"
        );

        let w_cpu = worker_cpu.load(Ordering::SeqCst);
        let b_cpu = blocking_cpu.load(Ordering::SeqCst);

        println!("Worker CPU: {}, Blocking CPU: {}", w_cpu, b_cpu);

        // Note: On some systems, blocking threads may still be allowed to run
        // on any CPU. The important thing is that they CAN run on different cores.
        // A strict test would require the system to have >1 CPU.
        if w_cpu != usize::MAX && b_cpu != usize::MAX {
            // If the system has proper affinity setup, blocking thread should
            // ideally be on a different core. Log a warning if they're on the same core.
            if w_cpu == b_cpu {
                println!(
                    " Warning: Blocking thread on same CPU as worker ({}) - this may impact latency",
                    w_cpu
                );
            } else {
                println!(
                    " Blocking thread ({}) runs on different CPU than worker ({})",
                    b_cpu, w_cpu
                );
            }
        }
    }

    /// Test that multiple workers run on different cores when configured.
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn multi_worker_core_isolation() {
        let rt = tokio::runtime::Builder::new_dpdk()
            .worker_threads(2) // Request 2 workers
            .enable_all()
            .build()
            .expect("Failed to create multi-worker DPDK runtime");

        let cpu_set: StdArc<std::sync::Mutex<std::collections::HashSet<usize>>> =
            StdArc::new(std::sync::Mutex::new(std::collections::HashSet::new()));

        let test_completed = StdArc::new(AtomicBool::new(false));
        let cpu_set_clone = cpu_set.clone();
        let test_completed_clone = test_completed.clone();

        rt.block_on(async move {
            let workers = tokio::runtime::dpdk::workers();
            assert!(
                workers.len() >= 2,
                "Multi-worker test requires at least 2 workers, got {}",
                workers.len()
            );

            // Spawn multiple tasks to check which CPUs they run on
            let mut handles = vec![];

            for _ in 0..10 {
                let cpu_set_inner = cpu_set_clone.clone();
                handles.push(tokio::spawn(async move {
                    let pid = Pid::from_raw(0);
                    if let Ok(cpuset) = sched_getaffinity(pid) {
                        for cpu in 0..CpuSet::count() {
                            if cpuset.is_set(cpu).unwrap_or(false) {
                                cpu_set_inner.lock().unwrap().insert(cpu);
                            }
                        }
                    }
                    // Add small delay to allow task distribution
                    tokio::time::sleep(Duration::from_millis(1)).await;
                }));
            }

            for handle in handles {
                handle.await.unwrap();
            }

            test_completed_clone.store(true, Ordering::SeqCst);
        });

        assert!(
            test_completed.load(Ordering::SeqCst),
            "Test should complete"
        );

        let cpus = cpu_set.lock().unwrap();
        println!("Tasks ran on CPUs: {:?}", cpus);

        // With multi-worker, tasks should potentially run on multiple cores
        // (depending on system configuration and actual number of DPDK cores available)
        assert!(
            !cpus.is_empty(),
            "At least one CPU should be detected for task execution"
        );

        println!(
            " Multi-worker test completed, {} unique CPU(s) observed",
            cpus.len()
        );
    }
}

#[cfg(all(target_os = "linux", feature = "full"))]
mod multi_ip_api_tests {
    use super::*;
    use tokio::runtime::dpdk;

    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn test_worker_ipv4s_ipv6s_consistency() {
        let rt = dpdk_rt();
        rt.block_on(async {
            let workers = dpdk::workers();

            for worker in workers {
                let ipv4s = dpdk::worker_ipv4s(worker);
                let ipv6s = dpdk::worker_ipv6s(worker);

                // Verify plural API returns Vec (may be empty but always valid)

                // Verify singular API returns first element (if any)
                assert_eq!(dpdk::worker_ipv4(worker), ipv4s.first().copied());
                assert_eq!(dpdk::worker_ipv6(worker), ipv6s.first().copied());
            }
        });
    }

    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn test_current_ipv4s_ipv6s() {
        let rt = dpdk_rt();
        rt.block_on(async {
            // Test from worker thread
            tokio::spawn(async {
                let current = dpdk::current_worker();

                let ipv4s_current = dpdk::current_ipv4s();
                let ipv4s_worker = dpdk::worker_ipv4s(current);
                assert_eq!(ipv4s_current, ipv4s_worker);

                let ipv6s_current = dpdk::current_ipv6s();
                let ipv6s_worker = dpdk::worker_ipv6s(current);
                assert_eq!(ipv6s_current, ipv6s_worker);
            }).await.unwrap();
        });
    }

    // Note: Panic test for calling outside runtime is omitted because:
    // 1. The API is already documented to panic outside DPDK runtime context
    // 2. Serial isolation tests make this difficult to test reliably
    // 3. The behavior is consistent with other dpdk:: APIs like current_worker()
}

// =============================================================================
// take_error() Tests  Two-Phase Error Detection
// =============================================================================

mod take_error_tests {
    use super::*;
    use tokio::net::{TcpDpdkStream, TcpListener};

    /// Healthy connection  take_error() returns None
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn take_error_healthy_connection() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            let server_task = tokio::spawn(async move {
                let (mut stream, _) = listener.accept().await.unwrap();
                stream.write_all(b"hello").await.unwrap();
                tokio::time::sleep(Duration::from_secs(2)).await;
            });

            let stream = TcpDpdkStream::connect(server_addr).await.unwrap();

            // On a healthy connection, take_error should return None
            let err = stream.take_error().expect("take_error() itself failed");
            assert!(
                err.is_none(),
                "take_error on healthy connection should be None, got: {:?}",
                err
            );

            server_task.abort();
        });
    }

    /// After we call shutdown(Write), take_error() returns None
    /// even if socket transitions to Closed (because we_closed flag is set)
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn take_error_after_write_shutdown() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            let server_task = tokio::spawn(async move {
                let (_stream, _) = listener.accept().await.unwrap();
                tokio::time::sleep(Duration::from_secs(2)).await;
            });

            let mut stream = TcpDpdkStream::connect(server_addr).await.unwrap();

            // Shutdown write side  sets write_shutdown flag
            stream.shutdown().await.expect("shutdown failed");

            // Wait for socket state to settle
            tokio::time::sleep(Duration::from_millis(200)).await;

            // take_error should return None because we initiated the close
            let err = stream.take_error().expect("take_error() itself failed");
            assert!(
                err.is_none(),
                "take_error after our shutdown should be None, got: {:?}",
                err
            );

            server_task.abort();
        });
    }

    /// Stored error with take semantics:
    /// try_write to a closed connection stores BrokenPipe 
    /// first take_error() returns BrokenPipe 
    /// second take_error() probes socket state instead
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn take_error_stored_error_take_semantics() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            // Server accepts and immediately drops  sends FIN
            let server_task = tokio::spawn(async move {
                let (_stream, _) = listener.accept().await.unwrap();
                // Drop immediately to trigger FIN
            });

            let stream = TcpDpdkStream::connect(server_addr).await.unwrap();
            server_task.await.unwrap();

            // Wait for FIN processing and RST cycle:
            // 1. Server drops  FIN sent  client enters CloseWait
            // 2. We try_write  data sent to server  server kernel sends RST
            // 3. smoltcp processes RST  socket enters Closed
            //
            // We need to write data to provoke the RST, then wait for processing.
            for _ in 0..10 {
                tokio::time::sleep(Duration::from_millis(100)).await;
                let _ = stream.try_write(b"probe");
            }

            // By now the socket should be Closed. try_write should fail with BrokenPipe
            // and store the error.
            match stream.try_write(b"should fail") {
                Err(ref e) if e.kind() == std::io::ErrorKind::BrokenPipe => {
                    // Expected: BrokenPipe stored via store_error()
                }
                Err(ref e) if e.kind() == std::io::ErrorKind::WouldBlock => {
                    // Socket might not be fully closed yet  skip assertion
                    return;
                }
                Ok(_) => {
                    // Socket still accepting writes  skip assertion
                    return;
                }
                Err(e) => {
                    panic!(
                        "Unexpected error kind from try_write: {:?} ({})",
                        e.kind(),
                        e
                    );
                }
            }

            // First take_error: should return stored BrokenPipe
            let first = stream.take_error().expect("take_error failed");
            assert!(first.is_some(), "first take_error should return stored error");
            assert_eq!(
                first.unwrap().kind(),
                std::io::ErrorKind::BrokenPipe,
                "stored error should be BrokenPipe"
            );

            // Second take_error: stored error was taken, falls through to state probe.
            // Socket is Closed and we didn't initiate shutdown  ConnectionReset
            let second = stream.take_error().expect("take_error failed");
            assert!(
                second.is_some(),
                "second take_error should detect closed state"
            );
            assert_eq!(
                second.unwrap().kind(),
                std::io::ErrorKind::ConnectionReset,
                "state probe should return ConnectionReset"
            );
        });
    }

    /// Peer closes without us writing  state probe detects ConnectionReset
    /// (no stored error because no I/O operation failed)
    #[serial_isolation_test::serial_isolation_test]
    #[test]
    fn take_error_peer_reset_detection() {
        let rt = dpdk_rt();
        let kernel_ip = get_kernel_ip();
        let port = get_test_port();
        let addr = format!("{}:{}", kernel_ip, port);

        rt.block_on(async {
            let listener = TcpListener::bind(&addr).await.unwrap();
            let server_addr = listener.local_addr().unwrap();

            // Server sends data then drops  FIN followed by possible RST
            let server_task = tokio::spawn(async move {
                let (mut stream, _) = listener.accept().await.unwrap();
                stream.write_all(b"DATA").await.unwrap();
                // Drop  FIN
            });

            let stream = TcpDpdkStream::connect(server_addr).await.unwrap();
            server_task.await.unwrap();

            // Read the data the server sent
            stream.readable().await.unwrap();
            let mut buf = [0u8; 64];
            let _ = stream.try_read(&mut buf);

            // Write data to provoke RST from server kernel (server process exited)
            // Then yield to let smoltcp process the RST
            for _ in 0..15 {
                let _ = stream.try_write(b"trigger RST");
                tokio::time::sleep(Duration::from_millis(100)).await;
            }

            // Check take_error  socket should be Closed, we_closed is false
            let err = stream.take_error().expect("take_error failed");
            if let Some(e) = err {
                // ConnectionReset is the expected result from state probe
                assert_eq!(
                    e.kind(),
                    std::io::ErrorKind::ConnectionReset,
                    "expected ConnectionReset from peer close, got: {:?}",
                    e.kind()
                );
            }
            // If None, the socket hasn't fully transitioned to Closed yet.
            // This is timing-dependent; the test passes either way since it's
            // testing the detection mechanism, not guaranteed timing.
        });
    }
}
