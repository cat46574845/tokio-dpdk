/* automatically generated by rust-bindgen 0.72.1 */

#[repr(C)]
#[derive(Copy, Clone, Debug, Default, Eq, Hash, Ord, PartialEq, PartialOrd)]
pub struct __BindgenBitfieldUnit<Storage> {
    storage: Storage,
}
impl<Storage> __BindgenBitfieldUnit<Storage> {
    #[inline]
    pub const fn new(storage: Storage) -> Self {
        Self { storage }
    }
}
impl<Storage> __BindgenBitfieldUnit<Storage>
where
    Storage: AsRef<[u8]> + AsMut<[u8]>,
{
    #[inline]
    fn extract_bit(byte: u8, index: usize) -> bool {
        let bit_index = if cfg!(target_endian = "big") {
            7 - (index % 8)
        } else {
            index % 8
        };
        let mask = 1 << bit_index;
        byte & mask == mask
    }
    #[inline]
    pub fn get_bit(&self, index: usize) -> bool {
        debug_assert!(index / 8 < self.storage.as_ref().len());
        let byte_index = index / 8;
        let byte = self.storage.as_ref()[byte_index];
        Self::extract_bit(byte, index)
    }
    #[inline]
    pub unsafe fn raw_get_bit(this: *const Self, index: usize) -> bool {
        debug_assert!(index / 8 < core::mem::size_of::<Storage>());
        let byte_index = index / 8;
        let byte = unsafe {
            *(core::ptr::addr_of!((*this).storage) as *const u8).offset(byte_index as isize)
        };
        Self::extract_bit(byte, index)
    }
    #[inline]
    fn change_bit(byte: u8, index: usize, val: bool) -> u8 {
        let bit_index = if cfg!(target_endian = "big") {
            7 - (index % 8)
        } else {
            index % 8
        };
        let mask = 1 << bit_index;
        if val {
            byte | mask
        } else {
            byte & !mask
        }
    }
    #[inline]
    pub fn set_bit(&mut self, index: usize, val: bool) {
        debug_assert!(index / 8 < self.storage.as_ref().len());
        let byte_index = index / 8;
        let byte = &mut self.storage.as_mut()[byte_index];
        *byte = Self::change_bit(*byte, index, val);
    }
    #[inline]
    pub unsafe fn raw_set_bit(this: *mut Self, index: usize, val: bool) {
        debug_assert!(index / 8 < core::mem::size_of::<Storage>());
        let byte_index = index / 8;
        let byte = unsafe {
            (core::ptr::addr_of_mut!((*this).storage) as *mut u8).offset(byte_index as isize)
        };
        unsafe { *byte = Self::change_bit(*byte, index, val) };
    }
    #[inline]
    pub fn get(&self, bit_offset: usize, bit_width: u8) -> u64 {
        debug_assert!(bit_width <= 64);
        debug_assert!(bit_offset / 8 < self.storage.as_ref().len());
        debug_assert!((bit_offset + (bit_width as usize)) / 8 <= self.storage.as_ref().len());
        let mut val = 0;
        for i in 0..(bit_width as usize) {
            if self.get_bit(i + bit_offset) {
                let index = if cfg!(target_endian = "big") {
                    bit_width as usize - 1 - i
                } else {
                    i
                };
                val |= 1 << index;
            }
        }
        val
    }
    #[inline]
    pub unsafe fn raw_get(this: *const Self, bit_offset: usize, bit_width: u8) -> u64 {
        debug_assert!(bit_width <= 64);
        debug_assert!(bit_offset / 8 < core::mem::size_of::<Storage>());
        debug_assert!((bit_offset + (bit_width as usize)) / 8 <= core::mem::size_of::<Storage>());
        let mut val = 0;
        for i in 0..(bit_width as usize) {
            if unsafe { Self::raw_get_bit(this, i + bit_offset) } {
                let index = if cfg!(target_endian = "big") {
                    bit_width as usize - 1 - i
                } else {
                    i
                };
                val |= 1 << index;
            }
        }
        val
    }
    #[inline]
    pub fn set(&mut self, bit_offset: usize, bit_width: u8, val: u64) {
        debug_assert!(bit_width <= 64);
        debug_assert!(bit_offset / 8 < self.storage.as_ref().len());
        debug_assert!((bit_offset + (bit_width as usize)) / 8 <= self.storage.as_ref().len());
        for i in 0..(bit_width as usize) {
            let mask = 1 << i;
            let val_bit_is_set = val & mask == mask;
            let index = if cfg!(target_endian = "big") {
                bit_width as usize - 1 - i
            } else {
                i
            };
            self.set_bit(index + bit_offset, val_bit_is_set);
        }
    }
    #[inline]
    pub unsafe fn raw_set(this: *mut Self, bit_offset: usize, bit_width: u8, val: u64) {
        debug_assert!(bit_width <= 64);
        debug_assert!(bit_offset / 8 < core::mem::size_of::<Storage>());
        debug_assert!((bit_offset + (bit_width as usize)) / 8 <= core::mem::size_of::<Storage>());
        for i in 0..(bit_width as usize) {
            let mask = 1 << i;
            let val_bit_is_set = val & mask == mask;
            let index = if cfg!(target_endian = "big") {
                bit_width as usize - 1 - i
            } else {
                i
            };
            unsafe { Self::raw_set_bit(this, index + bit_offset, val_bit_is_set) };
        }
    }
}
unsafe extern "C" {
    #[doc = " Initialize the Environment Abstraction Layer (EAL).\n\n This function is to be executed on the MAIN lcore only, as soon\n as possible in the application's main() function.\n It puts the WORKER lcores in the WAIT state.\n\n @param argc\n   A non-negative value.  If it is greater than 0, the array members\n   for argv[0] through argv[argc] (non-inclusive) shall contain pointers\n   to strings.\n @param argv\n   An array of strings.  The contents of the array, as well as the strings\n   which are pointed to by the array, may be modified by this function.\n   The program name pointer argv[0] is copied into the last parsed argv\n   so that argv[0] is still the same after deducing the parsed arguments.\n @return\n   - On success, the number of parsed arguments, which is greater or\n     equal to zero. After the call to rte_eal_init(),\n     all arguments argv[x] with x < ret may have been modified by this\n     function call and should not be further interpreted by the\n     application.  The EAL does not take any ownership of the memory used\n     for either the argv array, or its members.\n   - On failure, -1 and rte_errno is set to a value indicating the cause\n     for failure.  In some instances, the application will need to be\n     restarted as part of clearing the issue.\n\n   Error codes returned via rte_errno:\n     EACCES indicates a permissions issue.\n\n     EAGAIN indicates either a bus or system resource was not available,\n            setup may be attempted again.\n\n     EALREADY indicates that the rte_eal_init function has already been\n              called, and cannot be called again.\n\n     EFAULT indicates the tailq configuration name was not found in\n            memory configuration.\n\n     EINVAL indicates invalid parameters were passed as argv/argc.\n\n     ENOMEM indicates failure likely caused by an out-of-memory condition.\n\n     ENODEV indicates memory setup issues.\n\n     ENOTSUP indicates that the EAL cannot initialize on this system.\n\n     EPROTO indicates that the PCI bus is either not present, or is not\n            readable by the eal.\n\n     ENOEXEC indicates that a service core failed to launch successfully."]
    pub fn rte_eal_init(
        argc: ::core::ffi::c_int,
        argv: *mut *mut ::core::ffi::c_char,
    ) -> ::core::ffi::c_int;
}
unsafe extern "C" {
    #[doc = " Clean up the Environment Abstraction Layer (EAL)\n\n This function must be called to release any internal resources that EAL has\n allocated during rte_eal_init(). After this call, no DPDK function calls may\n be made. It is expected that common usage of this function is to call it\n just before terminating the process.\n\n @return\n  - 0 Successfully released all internal EAL resources.\n  - -EFAULT There was an error in releasing all resources."]
    pub fn rte_eal_cleanup() -> ::core::ffi::c_int;
}
#[doc = " IO virtual address type.\n When the physical addressing mode (IOVA as PA) is in use,\n the translation from an IO virtual address (IOVA) to a physical address\n is a direct mapping, i.e. the same value.\n Otherwise, in virtual mode (IOVA as VA), an IOMMU may do the translation."]
pub type rte_iova_t = u64;
#[doc = " Generic marker for any place in a structure."]
pub type RTE_MARKER = [*mut ::core::ffi::c_void; 0usize];
#[doc = " Marker for 8B alignment in a structure."]
pub type RTE_MARKER64 = [u64; 0usize];
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_device {
    _unused: [u8; 0],
}
unsafe extern "C" {
    #[doc = " Return the ID of the physical socket of the logical core we are\n running on.\n @return\n   the ID of current lcoreid's physical socket"]
    pub fn rte_socket_id() -> ::core::ffi::c_uint;
}
#[doc = " A structure used to configure the ring threshold registers of an Rx/Tx\n queue for an Ethernet port."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_thresh {
    #[doc = "< Ring prefetch threshold."]
    pub pthresh: u8,
    #[doc = "< Ring host threshold."]
    pub hthresh: u8,
    #[doc = "< Ring writeback threshold."]
    pub wthresh: u8,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_thresh"][::core::mem::size_of::<rte_eth_thresh>() - 3usize];
    ["Alignment of rte_eth_thresh"][::core::mem::align_of::<rte_eth_thresh>() - 1usize];
    ["Offset of field: rte_eth_thresh::pthresh"]
        [::core::mem::offset_of!(rte_eth_thresh, pthresh) - 0usize];
    ["Offset of field: rte_eth_thresh::hthresh"]
        [::core::mem::offset_of!(rte_eth_thresh, hthresh) - 1usize];
    ["Offset of field: rte_eth_thresh::wthresh"]
        [::core::mem::offset_of!(rte_eth_thresh, wthresh) - 2usize];
};
#[doc = " None of DCB, RSS or VMDq mode"]
pub const rte_eth_rx_mq_mode_RTE_ETH_MQ_RX_NONE: rte_eth_rx_mq_mode = 0;
#[doc = " For Rx side, only RSS is on"]
pub const rte_eth_rx_mq_mode_RTE_ETH_MQ_RX_RSS: rte_eth_rx_mq_mode = 1;
#[doc = " For Rx side,only DCB is on."]
pub const rte_eth_rx_mq_mode_RTE_ETH_MQ_RX_DCB: rte_eth_rx_mq_mode = 2;
#[doc = " Both DCB and RSS enable"]
pub const rte_eth_rx_mq_mode_RTE_ETH_MQ_RX_DCB_RSS: rte_eth_rx_mq_mode = 3;
#[doc = " Only VMDq, no RSS nor DCB"]
pub const rte_eth_rx_mq_mode_RTE_ETH_MQ_RX_VMDQ_ONLY: rte_eth_rx_mq_mode = 4;
#[doc = " RSS mode with VMDq"]
pub const rte_eth_rx_mq_mode_RTE_ETH_MQ_RX_VMDQ_RSS: rte_eth_rx_mq_mode = 5;
#[doc = " Use VMDq+DCB to route traffic to queues"]
pub const rte_eth_rx_mq_mode_RTE_ETH_MQ_RX_VMDQ_DCB: rte_eth_rx_mq_mode = 6;
#[doc = " Enable both VMDq and DCB in VMDq"]
pub const rte_eth_rx_mq_mode_RTE_ETH_MQ_RX_VMDQ_DCB_RSS: rte_eth_rx_mq_mode = 7;
#[doc = "  A set of values to identify what method is to be used to route\n  packets to multiple queues."]
pub type rte_eth_rx_mq_mode = ::core::ffi::c_int;
#[doc = "< It is in neither DCB nor VT mode."]
pub const rte_eth_tx_mq_mode_RTE_ETH_MQ_TX_NONE: rte_eth_tx_mq_mode = 0;
#[doc = "< For Tx side,only DCB is on."]
pub const rte_eth_tx_mq_mode_RTE_ETH_MQ_TX_DCB: rte_eth_tx_mq_mode = 1;
#[doc = "< For Tx side,both DCB and VT is on."]
pub const rte_eth_tx_mq_mode_RTE_ETH_MQ_TX_VMDQ_DCB: rte_eth_tx_mq_mode = 2;
#[doc = "< Only VT on, no DCB"]
pub const rte_eth_tx_mq_mode_RTE_ETH_MQ_TX_VMDQ_ONLY: rte_eth_tx_mq_mode = 3;
#[doc = " A set of values to identify what method is to be used to transmit\n packets using multi-TCs."]
pub type rte_eth_tx_mq_mode = ::core::ffi::c_int;
#[doc = " A structure used to configure the Rx features of an Ethernet port."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_rxmode {
    #[doc = " The multi-queue packet distribution mode to be used, e.g. RSS."]
    pub mq_mode: rte_eth_rx_mq_mode,
    #[doc = "< Requested MTU."]
    pub mtu: u32,
    #[doc = " Maximum allowed size of LRO aggregated packet."]
    pub max_lro_pkt_size: u32,
    #[doc = " Per-port Rx offloads to be set using RTE_ETH_RX_OFFLOAD_* flags.\n Only offloads set on rx_offload_capa field on rte_eth_dev_info\n structure are allowed to be set."]
    pub offloads: u64,
    #[doc = "< Reserved for future fields"]
    pub reserved_64s: [u64; 2usize],
    #[doc = "< Reserved for future fields"]
    pub reserved_ptrs: [*mut ::core::ffi::c_void; 2usize],
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_rxmode"][::core::mem::size_of::<rte_eth_rxmode>() - 56usize];
    ["Alignment of rte_eth_rxmode"][::core::mem::align_of::<rte_eth_rxmode>() - 8usize];
    ["Offset of field: rte_eth_rxmode::mq_mode"]
        [::core::mem::offset_of!(rte_eth_rxmode, mq_mode) - 0usize];
    ["Offset of field: rte_eth_rxmode::mtu"][::core::mem::offset_of!(rte_eth_rxmode, mtu) - 4usize];
    ["Offset of field: rte_eth_rxmode::max_lro_pkt_size"]
        [::core::mem::offset_of!(rte_eth_rxmode, max_lro_pkt_size) - 8usize];
    ["Offset of field: rte_eth_rxmode::offloads"]
        [::core::mem::offset_of!(rte_eth_rxmode, offloads) - 16usize];
    ["Offset of field: rte_eth_rxmode::reserved_64s"]
        [::core::mem::offset_of!(rte_eth_rxmode, reserved_64s) - 24usize];
    ["Offset of field: rte_eth_rxmode::reserved_ptrs"]
        [::core::mem::offset_of!(rte_eth_rxmode, reserved_ptrs) - 40usize];
};
#[doc = " DEFAULT means driver decides which hash algorithm to pick."]
pub const rte_eth_hash_function_RTE_ETH_HASH_FUNCTION_DEFAULT: rte_eth_hash_function = 0;
#[doc = "< Toeplitz"]
pub const rte_eth_hash_function_RTE_ETH_HASH_FUNCTION_TOEPLITZ: rte_eth_hash_function = 1;
#[doc = "< Simple XOR"]
pub const rte_eth_hash_function_RTE_ETH_HASH_FUNCTION_SIMPLE_XOR: rte_eth_hash_function = 2;
#[doc = " Symmetric Toeplitz: src, dst will be replaced by\n xor(src, dst). For the case with src/dst only,\n src or dst address will xor with zero pair."]
pub const rte_eth_hash_function_RTE_ETH_HASH_FUNCTION_SYMMETRIC_TOEPLITZ: rte_eth_hash_function = 3;
#[doc = " Symmetric Toeplitz: L3 and L4 fields are sorted prior to\n the hash function.\n  If src_ip > dst_ip, swap src_ip and dst_ip.\n  If src_port > dst_port, swap src_port and dst_port."]
pub const rte_eth_hash_function_RTE_ETH_HASH_FUNCTION_SYMMETRIC_TOEPLITZ_SORT:
    rte_eth_hash_function = 4;
#[doc = " Symmetric Toeplitz: L3 and L4 fields are sorted prior to\n the hash function.\n  If src_ip > dst_ip, swap src_ip and dst_ip.\n  If src_port > dst_port, swap src_port and dst_port."]
pub const rte_eth_hash_function_RTE_ETH_HASH_FUNCTION_MAX: rte_eth_hash_function = 5;
#[doc = " Hash function types."]
pub type rte_eth_hash_function = ::core::ffi::c_int;
#[doc = " A structure used to configure the Receive Side Scaling (RSS) feature\n of an Ethernet port."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_rss_conf {
    #[doc = " In rte_eth_dev_rss_hash_conf_get(), the *rss_key_len* should be\n greater than or equal to the *hash_key_size* which get from\n rte_eth_dev_info_get() API. And the *rss_key* should contain at least\n *hash_key_size* bytes. If not meet these requirements, the query\n result is unreliable even if the operation returns success.\n\n In rte_eth_dev_rss_hash_update() or rte_eth_dev_configure(), if\n *rss_key* is not NULL, the *rss_key_len* indicates the length of the\n *rss_key* in bytes and it should be equal to *hash_key_size*.\n If *rss_key* is NULL, drivers are free to use a random or a default key."]
    pub rss_key: *mut u8,
    #[doc = "< hash key length in bytes."]
    pub rss_key_len: u8,
    #[doc = " Indicates the type of packets or the specific part of packets to\n which RSS hashing is to be applied."]
    pub rss_hf: u64,
    #[doc = "< Hash algorithm."]
    pub algorithm: rte_eth_hash_function,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_rss_conf"][::core::mem::size_of::<rte_eth_rss_conf>() - 32usize];
    ["Alignment of rte_eth_rss_conf"][::core::mem::align_of::<rte_eth_rss_conf>() - 8usize];
    ["Offset of field: rte_eth_rss_conf::rss_key"]
        [::core::mem::offset_of!(rte_eth_rss_conf, rss_key) - 0usize];
    ["Offset of field: rte_eth_rss_conf::rss_key_len"]
        [::core::mem::offset_of!(rte_eth_rss_conf, rss_key_len) - 8usize];
    ["Offset of field: rte_eth_rss_conf::rss_hf"]
        [::core::mem::offset_of!(rte_eth_rss_conf, rss_hf) - 16usize];
    ["Offset of field: rte_eth_rss_conf::algorithm"]
        [::core::mem::offset_of!(rte_eth_rss_conf, algorithm) - 24usize];
};
#[doc = "< 4 TCs with DCB."]
pub const rte_eth_nb_tcs_RTE_ETH_4_TCS: rte_eth_nb_tcs = 4;
#[doc = "< 8 TCs with DCB."]
pub const rte_eth_nb_tcs_RTE_ETH_8_TCS: rte_eth_nb_tcs = 8;
#[doc = " This enum indicates the possible number of traffic classes\n in DCB configurations"]
pub type rte_eth_nb_tcs = ::core::ffi::c_int;
#[doc = "< 8 VMDq pools."]
pub const rte_eth_nb_pools_RTE_ETH_8_POOLS: rte_eth_nb_pools = 8;
#[doc = "< 16 VMDq pools."]
pub const rte_eth_nb_pools_RTE_ETH_16_POOLS: rte_eth_nb_pools = 16;
#[doc = "< 32 VMDq pools."]
pub const rte_eth_nb_pools_RTE_ETH_32_POOLS: rte_eth_nb_pools = 32;
#[doc = "< 64 VMDq pools."]
pub const rte_eth_nb_pools_RTE_ETH_64_POOLS: rte_eth_nb_pools = 64;
#[doc = " This enum indicates the possible number of queue pools\n in VMDq configurations."]
pub type rte_eth_nb_pools = ::core::ffi::c_int;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_dcb_rx_conf {
    #[doc = "< Possible DCB TCs, 4 or 8 TCs"]
    pub nb_tcs: rte_eth_nb_tcs,
    #[doc = " Traffic class each UP mapped to."]
    pub dcb_tc: [u8; 8usize],
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_dcb_rx_conf"][::core::mem::size_of::<rte_eth_dcb_rx_conf>() - 12usize];
    ["Alignment of rte_eth_dcb_rx_conf"][::core::mem::align_of::<rte_eth_dcb_rx_conf>() - 4usize];
    ["Offset of field: rte_eth_dcb_rx_conf::nb_tcs"]
        [::core::mem::offset_of!(rte_eth_dcb_rx_conf, nb_tcs) - 0usize];
    ["Offset of field: rte_eth_dcb_rx_conf::dcb_tc"]
        [::core::mem::offset_of!(rte_eth_dcb_rx_conf, dcb_tc) - 4usize];
};
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_vmdq_dcb_tx_conf {
    #[doc = "< With DCB, 16 or 32 pools."]
    pub nb_queue_pools: rte_eth_nb_pools,
    #[doc = " Traffic class each UP mapped to."]
    pub dcb_tc: [u8; 8usize],
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_vmdq_dcb_tx_conf"]
        [::core::mem::size_of::<rte_eth_vmdq_dcb_tx_conf>() - 12usize];
    ["Alignment of rte_eth_vmdq_dcb_tx_conf"]
        [::core::mem::align_of::<rte_eth_vmdq_dcb_tx_conf>() - 4usize];
    ["Offset of field: rte_eth_vmdq_dcb_tx_conf::nb_queue_pools"]
        [::core::mem::offset_of!(rte_eth_vmdq_dcb_tx_conf, nb_queue_pools) - 0usize];
    ["Offset of field: rte_eth_vmdq_dcb_tx_conf::dcb_tc"]
        [::core::mem::offset_of!(rte_eth_vmdq_dcb_tx_conf, dcb_tc) - 4usize];
};
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_dcb_tx_conf {
    #[doc = "< Possible DCB TCs, 4 or 8 TCs."]
    pub nb_tcs: rte_eth_nb_tcs,
    #[doc = " Traffic class each UP mapped to."]
    pub dcb_tc: [u8; 8usize],
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_dcb_tx_conf"][::core::mem::size_of::<rte_eth_dcb_tx_conf>() - 12usize];
    ["Alignment of rte_eth_dcb_tx_conf"][::core::mem::align_of::<rte_eth_dcb_tx_conf>() - 4usize];
    ["Offset of field: rte_eth_dcb_tx_conf::nb_tcs"]
        [::core::mem::offset_of!(rte_eth_dcb_tx_conf, nb_tcs) - 0usize];
    ["Offset of field: rte_eth_dcb_tx_conf::dcb_tc"]
        [::core::mem::offset_of!(rte_eth_dcb_tx_conf, dcb_tc) - 4usize];
};
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_vmdq_tx_conf {
    #[doc = "< VMDq mode, 64 pools."]
    pub nb_queue_pools: rte_eth_nb_pools,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_vmdq_tx_conf"][::core::mem::size_of::<rte_eth_vmdq_tx_conf>() - 4usize];
    ["Alignment of rte_eth_vmdq_tx_conf"][::core::mem::align_of::<rte_eth_vmdq_tx_conf>() - 4usize];
    ["Offset of field: rte_eth_vmdq_tx_conf::nb_queue_pools"]
        [::core::mem::offset_of!(rte_eth_vmdq_tx_conf, nb_queue_pools) - 0usize];
};
#[doc = " A structure used to configure the VMDq+DCB feature\n of an Ethernet port.\n\n Using this feature, packets are routed to a pool of queues, based\n on the VLAN ID in the VLAN tag, and then to a specific queue within\n that pool, using the user priority VLAN tag field.\n\n A default pool may be used, if desired, to route all traffic which\n does not match the VLAN filter rules."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_vmdq_dcb_conf {
    #[doc = "< With DCB, 16 or 32 pools"]
    pub nb_queue_pools: rte_eth_nb_pools,
    #[doc = "< If non-zero, use a default pool"]
    pub enable_default_pool: u8,
    #[doc = "< The default pool, if applicable"]
    pub default_pool: u8,
    #[doc = "< We can have up to 64 filters/mappings"]
    pub nb_pool_maps: u8,
    #[doc = "< VMDq VLAN pool maps."]
    pub pool_map: [rte_eth_vmdq_dcb_conf__bindgen_ty_1; 64usize],
    #[doc = " Selects a queue in a pool"]
    pub dcb_tc: [u8; 8usize],
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_vmdq_dcb_conf__bindgen_ty_1 {
    #[doc = "< The VLAN ID of the received frame"]
    pub vlan_id: u16,
    #[doc = "< Bitmask of pools for packet Rx"]
    pub pools: u64,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_vmdq_dcb_conf__bindgen_ty_1"]
        [::core::mem::size_of::<rte_eth_vmdq_dcb_conf__bindgen_ty_1>() - 16usize];
    ["Alignment of rte_eth_vmdq_dcb_conf__bindgen_ty_1"]
        [::core::mem::align_of::<rte_eth_vmdq_dcb_conf__bindgen_ty_1>() - 8usize];
    ["Offset of field: rte_eth_vmdq_dcb_conf__bindgen_ty_1::vlan_id"]
        [::core::mem::offset_of!(rte_eth_vmdq_dcb_conf__bindgen_ty_1, vlan_id) - 0usize];
    ["Offset of field: rte_eth_vmdq_dcb_conf__bindgen_ty_1::pools"]
        [::core::mem::offset_of!(rte_eth_vmdq_dcb_conf__bindgen_ty_1, pools) - 8usize];
};
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_vmdq_dcb_conf"][::core::mem::size_of::<rte_eth_vmdq_dcb_conf>() - 1040usize];
    ["Alignment of rte_eth_vmdq_dcb_conf"]
        [::core::mem::align_of::<rte_eth_vmdq_dcb_conf>() - 8usize];
    ["Offset of field: rte_eth_vmdq_dcb_conf::nb_queue_pools"]
        [::core::mem::offset_of!(rte_eth_vmdq_dcb_conf, nb_queue_pools) - 0usize];
    ["Offset of field: rte_eth_vmdq_dcb_conf::enable_default_pool"]
        [::core::mem::offset_of!(rte_eth_vmdq_dcb_conf, enable_default_pool) - 4usize];
    ["Offset of field: rte_eth_vmdq_dcb_conf::default_pool"]
        [::core::mem::offset_of!(rte_eth_vmdq_dcb_conf, default_pool) - 5usize];
    ["Offset of field: rte_eth_vmdq_dcb_conf::nb_pool_maps"]
        [::core::mem::offset_of!(rte_eth_vmdq_dcb_conf, nb_pool_maps) - 6usize];
    ["Offset of field: rte_eth_vmdq_dcb_conf::pool_map"]
        [::core::mem::offset_of!(rte_eth_vmdq_dcb_conf, pool_map) - 8usize];
    ["Offset of field: rte_eth_vmdq_dcb_conf::dcb_tc"]
        [::core::mem::offset_of!(rte_eth_vmdq_dcb_conf, dcb_tc) - 1032usize];
};
#[doc = " A structure used to configure the VMDq feature of an Ethernet port when\n not combined with the DCB feature.\n\n Using this feature, packets are routed to a pool of queues. By default,\n the pool selection is based on the MAC address, the VLAN ID in the\n VLAN tag as specified in the pool_map array.\n Passing the RTE_ETH_VMDQ_ACCEPT_UNTAG in the rx_mode field allows pool\n selection using only the MAC address. MAC address to pool mapping is done\n using the rte_eth_dev_mac_addr_add function, with the pool parameter\n corresponding to the pool ID.\n\n Queue selection within the selected pool will be done using RSS when\n it is enabled or revert to the first queue of the pool if not.\n\n A default pool may be used, if desired, to route all traffic which\n does not match the VLAN filter rules or any pool MAC address."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_vmdq_rx_conf {
    #[doc = "< VMDq only mode, 8 or 64 pools"]
    pub nb_queue_pools: rte_eth_nb_pools,
    #[doc = "< If non-zero, use a default pool"]
    pub enable_default_pool: u8,
    #[doc = "< The default pool, if applicable"]
    pub default_pool: u8,
    #[doc = "< Enable VT loop back"]
    pub enable_loop_back: u8,
    #[doc = "< We can have up to 64 filters/mappings"]
    pub nb_pool_maps: u8,
    #[doc = "< Flags from RTE_ETH_VMDQ_ACCEPT_*"]
    pub rx_mode: u32,
    #[doc = "< VMDq VLAN pool maps."]
    pub pool_map: [rte_eth_vmdq_rx_conf__bindgen_ty_1; 64usize],
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_vmdq_rx_conf__bindgen_ty_1 {
    #[doc = "< The VLAN ID of the received frame"]
    pub vlan_id: u16,
    #[doc = "< Bitmask of pools for packet Rx"]
    pub pools: u64,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_vmdq_rx_conf__bindgen_ty_1"]
        [::core::mem::size_of::<rte_eth_vmdq_rx_conf__bindgen_ty_1>() - 16usize];
    ["Alignment of rte_eth_vmdq_rx_conf__bindgen_ty_1"]
        [::core::mem::align_of::<rte_eth_vmdq_rx_conf__bindgen_ty_1>() - 8usize];
    ["Offset of field: rte_eth_vmdq_rx_conf__bindgen_ty_1::vlan_id"]
        [::core::mem::offset_of!(rte_eth_vmdq_rx_conf__bindgen_ty_1, vlan_id) - 0usize];
    ["Offset of field: rte_eth_vmdq_rx_conf__bindgen_ty_1::pools"]
        [::core::mem::offset_of!(rte_eth_vmdq_rx_conf__bindgen_ty_1, pools) - 8usize];
};
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_vmdq_rx_conf"][::core::mem::size_of::<rte_eth_vmdq_rx_conf>() - 1040usize];
    ["Alignment of rte_eth_vmdq_rx_conf"][::core::mem::align_of::<rte_eth_vmdq_rx_conf>() - 8usize];
    ["Offset of field: rte_eth_vmdq_rx_conf::nb_queue_pools"]
        [::core::mem::offset_of!(rte_eth_vmdq_rx_conf, nb_queue_pools) - 0usize];
    ["Offset of field: rte_eth_vmdq_rx_conf::enable_default_pool"]
        [::core::mem::offset_of!(rte_eth_vmdq_rx_conf, enable_default_pool) - 4usize];
    ["Offset of field: rte_eth_vmdq_rx_conf::default_pool"]
        [::core::mem::offset_of!(rte_eth_vmdq_rx_conf, default_pool) - 5usize];
    ["Offset of field: rte_eth_vmdq_rx_conf::enable_loop_back"]
        [::core::mem::offset_of!(rte_eth_vmdq_rx_conf, enable_loop_back) - 6usize];
    ["Offset of field: rte_eth_vmdq_rx_conf::nb_pool_maps"]
        [::core::mem::offset_of!(rte_eth_vmdq_rx_conf, nb_pool_maps) - 7usize];
    ["Offset of field: rte_eth_vmdq_rx_conf::rx_mode"]
        [::core::mem::offset_of!(rte_eth_vmdq_rx_conf, rx_mode) - 8usize];
    ["Offset of field: rte_eth_vmdq_rx_conf::pool_map"]
        [::core::mem::offset_of!(rte_eth_vmdq_rx_conf, pool_map) - 16usize];
};
#[doc = " A structure used to configure the Tx features of an Ethernet port."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_txmode {
    #[doc = "< Tx multi-queues mode."]
    pub mq_mode: rte_eth_tx_mq_mode,
    #[doc = " Per-port Tx offloads to be set using RTE_ETH_TX_OFFLOAD_* flags.\n Only offloads set on tx_offload_capa field on rte_eth_dev_info\n structure are allowed to be set."]
    pub offloads: u64,
    pub pvid: u16,
    pub _bitfield_align_1: [u8; 0],
    pub _bitfield_1: __BindgenBitfieldUnit<[u8; 1usize]>,
    #[doc = "< Reserved for future fields"]
    pub reserved_64s: [u64; 2usize],
    #[doc = "< Reserved for future fields"]
    pub reserved_ptrs: [*mut ::core::ffi::c_void; 2usize],
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_txmode"][::core::mem::size_of::<rte_eth_txmode>() - 56usize];
    ["Alignment of rte_eth_txmode"][::core::mem::align_of::<rte_eth_txmode>() - 8usize];
    ["Offset of field: rte_eth_txmode::mq_mode"]
        [::core::mem::offset_of!(rte_eth_txmode, mq_mode) - 0usize];
    ["Offset of field: rte_eth_txmode::offloads"]
        [::core::mem::offset_of!(rte_eth_txmode, offloads) - 8usize];
    ["Offset of field: rte_eth_txmode::pvid"]
        [::core::mem::offset_of!(rte_eth_txmode, pvid) - 16usize];
    ["Offset of field: rte_eth_txmode::reserved_64s"]
        [::core::mem::offset_of!(rte_eth_txmode, reserved_64s) - 24usize];
    ["Offset of field: rte_eth_txmode::reserved_ptrs"]
        [::core::mem::offset_of!(rte_eth_txmode, reserved_ptrs) - 40usize];
};
impl rte_eth_txmode {
    #[inline]
    pub fn hw_vlan_reject_tagged(&self) -> u8 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(0usize, 1u8) as u8) }
    }
    #[inline]
    pub fn set_hw_vlan_reject_tagged(&mut self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            self._bitfield_1.set(0usize, 1u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn hw_vlan_reject_tagged_raw(this: *const Self) -> u8 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 1usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                0usize,
                1u8,
            ) as u8)
        }
    }
    #[inline]
    pub unsafe fn set_hw_vlan_reject_tagged_raw(this: *mut Self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 1usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                0usize,
                1u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn hw_vlan_reject_untagged(&self) -> u8 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(1usize, 1u8) as u8) }
    }
    #[inline]
    pub fn set_hw_vlan_reject_untagged(&mut self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            self._bitfield_1.set(1usize, 1u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn hw_vlan_reject_untagged_raw(this: *const Self) -> u8 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 1usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                1usize,
                1u8,
            ) as u8)
        }
    }
    #[inline]
    pub unsafe fn set_hw_vlan_reject_untagged_raw(this: *mut Self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 1usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                1usize,
                1u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn hw_vlan_insert_pvid(&self) -> u8 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(2usize, 1u8) as u8) }
    }
    #[inline]
    pub fn set_hw_vlan_insert_pvid(&mut self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            self._bitfield_1.set(2usize, 1u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn hw_vlan_insert_pvid_raw(this: *const Self) -> u8 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 1usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                2usize,
                1u8,
            ) as u8)
        }
    }
    #[inline]
    pub unsafe fn set_hw_vlan_insert_pvid_raw(this: *mut Self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 1usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                2usize,
                1u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn new_bitfield_1(
        hw_vlan_reject_tagged: u8,
        hw_vlan_reject_untagged: u8,
        hw_vlan_insert_pvid: u8,
    ) -> __BindgenBitfieldUnit<[u8; 1usize]> {
        let mut __bindgen_bitfield_unit: __BindgenBitfieldUnit<[u8; 1usize]> = Default::default();
        __bindgen_bitfield_unit.set(0usize, 1u8, {
            let hw_vlan_reject_tagged: u8 =
                unsafe { ::core::mem::transmute(hw_vlan_reject_tagged) };
            hw_vlan_reject_tagged as u64
        });
        __bindgen_bitfield_unit.set(1usize, 1u8, {
            let hw_vlan_reject_untagged: u8 =
                unsafe { ::core::mem::transmute(hw_vlan_reject_untagged) };
            hw_vlan_reject_untagged as u64
        });
        __bindgen_bitfield_unit.set(2usize, 1u8, {
            let hw_vlan_insert_pvid: u8 = unsafe { ::core::mem::transmute(hw_vlan_insert_pvid) };
            hw_vlan_insert_pvid as u64
        });
        __bindgen_bitfield_unit
    }
}
#[doc = " @warning\n @b EXPERIMENTAL: this structure may change without prior notice.\n\n A structure used to configure an Rx packet segment to split.\n\n If RTE_ETH_RX_OFFLOAD_BUFFER_SPLIT flag is set in offloads field,\n the PMD will split the received packets into multiple segments\n according to the specification in the description array:\n\n - The first network buffer will be allocated from the memory pool,\n   specified in the first array element, the second buffer, from the\n   pool in the second element, and so on.\n\n - The proto_hdrs in the elements define the split position of\n   received packets.\n\n - The offsets from the segment description elements specify\n   the data offset from the buffer beginning except the first mbuf.\n   The first segment offset is added with RTE_PKTMBUF_HEADROOM.\n\n - The lengths in the elements define the maximal data amount\n   being received to each segment. The receiving starts with filling\n   up the first mbuf data buffer up to specified length. If the\n   there are data remaining (packet is longer than buffer in the first\n   mbuf) the following data will be pushed to the next segment\n   up to its own length, and so on.\n\n - If the length in the segment description element is zero\n   the actual buffer size will be deduced from the appropriate\n   memory pool properties.\n\n - If there is not enough elements to describe the buffer for entire\n   packet of maximal length the following parameters will be used\n   for the all remaining segments:\n     - pool from the last valid element\n     - the buffer size from this pool\n     - zero offset\n\n - Length based buffer split:\n     - mp, length, offset should be configured.\n     - The proto_hdr field must be 0.\n\n - Protocol header based buffer split:\n     - mp, offset, proto_hdr should be configured.\n     - The length field must be 0.\n     - The proto_hdr field in the last segment should be 0.\n\n - When protocol header split is enabled, NIC may receive packets\n   which do not match all the protocol headers within the Rx segments.\n   At this point, NIC will have two possible split behaviors according to\n   matching results, one is exact match, another is longest match.\n   The split result of NIC must belong to one of them.\n   The exact match means NIC only do split when the packets exactly match all\n   the protocol headers in the segments.\n   Otherwise, the whole packet will be put into the last valid mempool.\n   The longest match means NIC will do split until packets mismatch\n   the protocol header in the segments.\n   The rest will be put into the last valid pool."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_rxseg_split {
    #[doc = "< Memory pool to allocate segment from."]
    pub mp: *mut rte_mempool,
    #[doc = "< Segment data length, configures split point."]
    pub length: u16,
    #[doc = "< Data offset from beginning of mbuf data buffer."]
    pub offset: u16,
    #[doc = " proto_hdr defines a bit mask of the protocol sequence as RTE_PTYPE_*.\n The last RTE_PTYPE* in the mask indicates the split position.\n\n If one protocol header is defined to split packets into two segments,\n for non-tunneling packets, the complete protocol sequence should be defined.\n For tunneling packets, for simplicity, only the tunnel and inner part of\n complete protocol sequence is required.\n If several protocol headers are defined to split packets into multi-segments,\n the repeated parts of adjacent segments should be omitted."]
    pub proto_hdr: u32,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_rxseg_split"][::core::mem::size_of::<rte_eth_rxseg_split>() - 16usize];
    ["Alignment of rte_eth_rxseg_split"][::core::mem::align_of::<rte_eth_rxseg_split>() - 8usize];
    ["Offset of field: rte_eth_rxseg_split::mp"]
        [::core::mem::offset_of!(rte_eth_rxseg_split, mp) - 0usize];
    ["Offset of field: rte_eth_rxseg_split::length"]
        [::core::mem::offset_of!(rte_eth_rxseg_split, length) - 8usize];
    ["Offset of field: rte_eth_rxseg_split::offset"]
        [::core::mem::offset_of!(rte_eth_rxseg_split, offset) - 10usize];
    ["Offset of field: rte_eth_rxseg_split::proto_hdr"]
        [::core::mem::offset_of!(rte_eth_rxseg_split, proto_hdr) - 12usize];
};
#[doc = " @warning\n @b EXPERIMENTAL: this structure may change without prior notice.\n\n A common structure used to describe Rx packet segment properties."]
#[repr(C)]
#[derive(Copy, Clone)]
pub union rte_eth_rxseg {
    pub split: rte_eth_rxseg_split,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_rxseg"][::core::mem::size_of::<rte_eth_rxseg>() - 16usize];
    ["Alignment of rte_eth_rxseg"][::core::mem::align_of::<rte_eth_rxseg>() - 8usize];
    ["Offset of field: rte_eth_rxseg::split"]
        [::core::mem::offset_of!(rte_eth_rxseg, split) - 0usize];
};
#[doc = " A structure used to configure an Rx ring of an Ethernet port."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_rxconf {
    #[doc = "< Rx ring threshold registers."]
    pub rx_thresh: rte_eth_thresh,
    #[doc = "< Drives the freeing of Rx descriptors."]
    pub rx_free_thresh: u16,
    #[doc = "< Drop packets if no descriptors are available."]
    pub rx_drop_en: u8,
    #[doc = "< Do not start queue with rte_eth_dev_start()."]
    pub rx_deferred_start: u8,
    #[doc = "< Number of descriptions in rx_seg array."]
    pub rx_nseg: u16,
    #[doc = " Share group index in Rx domain and switch domain.\n Non-zero value to enable Rx queue share, zero value disable share.\n PMD is responsible for Rx queue consistency checks to avoid member\n port's configuration contradict to each other."]
    pub share_group: u16,
    #[doc = "< Shared Rx queue ID in group"]
    pub share_qid: u16,
    #[doc = " Per-queue Rx offloads to be set using RTE_ETH_RX_OFFLOAD_* flags.\n Only offloads set on rx_queue_offload_capa or rx_offload_capa\n fields on rte_eth_dev_info structure are allowed to be set."]
    pub offloads: u64,
    #[doc = " Points to the array of segment descriptions for an entire packet.\n Array elements are properties for consecutive Rx segments.\n\n The supported capabilities of receiving segmentation is reported\n in rte_eth_dev_info.rx_seg_capa field."]
    pub rx_seg: *mut rte_eth_rxseg,
    #[doc = " Array of mempools to allocate Rx buffers from.\n\n This provides support for multiple mbuf pools per Rx queue.\n The capability is reported in device info via positive\n max_rx_mempools.\n\n It could be useful for more efficient usage of memory when an\n application creates different mempools to steer the specific\n size of the packet.\n\n If many mempools are specified, packets received using Rx\n burst may belong to any provided mempool. From ethdev user point\n of view it is undefined how PMD/NIC chooses mempool for a packet.\n\n If Rx scatter is enabled, a packet may be delivered using a chain\n of mbufs obtained from single mempool or multiple mempools based\n on the NIC implementation."]
    pub rx_mempools: *mut *mut rte_mempool,
    pub rx_nmempool: u16,
    #[doc = "< Reserved for future fields"]
    pub reserved_64s: [u64; 2usize],
    #[doc = "< Reserved for future fields"]
    pub reserved_ptrs: [*mut ::core::ffi::c_void; 2usize],
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_rxconf"][::core::mem::size_of::<rte_eth_rxconf>() - 80usize];
    ["Alignment of rte_eth_rxconf"][::core::mem::align_of::<rte_eth_rxconf>() - 8usize];
    ["Offset of field: rte_eth_rxconf::rx_thresh"]
        [::core::mem::offset_of!(rte_eth_rxconf, rx_thresh) - 0usize];
    ["Offset of field: rte_eth_rxconf::rx_free_thresh"]
        [::core::mem::offset_of!(rte_eth_rxconf, rx_free_thresh) - 4usize];
    ["Offset of field: rte_eth_rxconf::rx_drop_en"]
        [::core::mem::offset_of!(rte_eth_rxconf, rx_drop_en) - 6usize];
    ["Offset of field: rte_eth_rxconf::rx_deferred_start"]
        [::core::mem::offset_of!(rte_eth_rxconf, rx_deferred_start) - 7usize];
    ["Offset of field: rte_eth_rxconf::rx_nseg"]
        [::core::mem::offset_of!(rte_eth_rxconf, rx_nseg) - 8usize];
    ["Offset of field: rte_eth_rxconf::share_group"]
        [::core::mem::offset_of!(rte_eth_rxconf, share_group) - 10usize];
    ["Offset of field: rte_eth_rxconf::share_qid"]
        [::core::mem::offset_of!(rte_eth_rxconf, share_qid) - 12usize];
    ["Offset of field: rte_eth_rxconf::offloads"]
        [::core::mem::offset_of!(rte_eth_rxconf, offloads) - 16usize];
    ["Offset of field: rte_eth_rxconf::rx_seg"]
        [::core::mem::offset_of!(rte_eth_rxconf, rx_seg) - 24usize];
    ["Offset of field: rte_eth_rxconf::rx_mempools"]
        [::core::mem::offset_of!(rte_eth_rxconf, rx_mempools) - 32usize];
    ["Offset of field: rte_eth_rxconf::rx_nmempool"]
        [::core::mem::offset_of!(rte_eth_rxconf, rx_nmempool) - 40usize];
    ["Offset of field: rte_eth_rxconf::reserved_64s"]
        [::core::mem::offset_of!(rte_eth_rxconf, reserved_64s) - 48usize];
    ["Offset of field: rte_eth_rxconf::reserved_ptrs"]
        [::core::mem::offset_of!(rte_eth_rxconf, reserved_ptrs) - 64usize];
};
#[doc = " A structure used to configure a Tx ring of an Ethernet port."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_txconf {
    #[doc = "< Tx ring threshold registers."]
    pub tx_thresh: rte_eth_thresh,
    #[doc = "< Drives the setting of RS bit on TXDs."]
    pub tx_rs_thresh: u16,
    #[doc = "< Start freeing Tx buffers if there are\nless free descriptors than this value."]
    pub tx_free_thresh: u16,
    #[doc = "< Do not start queue with rte_eth_dev_start()."]
    pub tx_deferred_start: u8,
    #[doc = " Per-queue Tx offloads to be set  using RTE_ETH_TX_OFFLOAD_* flags.\n Only offloads set on tx_queue_offload_capa or tx_offload_capa\n fields on rte_eth_dev_info structure are allowed to be set."]
    pub offloads: u64,
    #[doc = "< Reserved for future fields"]
    pub reserved_64s: [u64; 2usize],
    #[doc = "< Reserved for future fields"]
    pub reserved_ptrs: [*mut ::core::ffi::c_void; 2usize],
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_txconf"][::core::mem::size_of::<rte_eth_txconf>() - 56usize];
    ["Alignment of rte_eth_txconf"][::core::mem::align_of::<rte_eth_txconf>() - 8usize];
    ["Offset of field: rte_eth_txconf::tx_thresh"]
        [::core::mem::offset_of!(rte_eth_txconf, tx_thresh) - 0usize];
    ["Offset of field: rte_eth_txconf::tx_rs_thresh"]
        [::core::mem::offset_of!(rte_eth_txconf, tx_rs_thresh) - 4usize];
    ["Offset of field: rte_eth_txconf::tx_free_thresh"]
        [::core::mem::offset_of!(rte_eth_txconf, tx_free_thresh) - 6usize];
    ["Offset of field: rte_eth_txconf::tx_deferred_start"]
        [::core::mem::offset_of!(rte_eth_txconf, tx_deferred_start) - 8usize];
    ["Offset of field: rte_eth_txconf::offloads"]
        [::core::mem::offset_of!(rte_eth_txconf, offloads) - 16usize];
    ["Offset of field: rte_eth_txconf::reserved_64s"]
        [::core::mem::offset_of!(rte_eth_txconf, reserved_64s) - 24usize];
    ["Offset of field: rte_eth_txconf::reserved_ptrs"]
        [::core::mem::offset_of!(rte_eth_txconf, reserved_ptrs) - 40usize];
};
#[doc = " A structure contains information about HW descriptor ring limitations."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_desc_lim {
    #[doc = "< Max allowed number of descriptors."]
    pub nb_max: u16,
    #[doc = "< Min allowed number of descriptors."]
    pub nb_min: u16,
    #[doc = "< Number of descriptors should be aligned to."]
    pub nb_align: u16,
    #[doc = " Max allowed number of segments per whole packet.\n\n - For TSO packet this is the total number of data descriptors allowed\n   by device.\n\n @see nb_mtu_seg_max"]
    pub nb_seg_max: u16,
    #[doc = " Max number of segments per one MTU.\n\n - For non-TSO packet, this is the maximum allowed number of segments\n   in a single transmit packet.\n\n - For TSO packet each segment within the TSO may span up to this\n   value.\n\n @see nb_seg_max"]
    pub nb_mtu_seg_max: u16,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_desc_lim"][::core::mem::size_of::<rte_eth_desc_lim>() - 10usize];
    ["Alignment of rte_eth_desc_lim"][::core::mem::align_of::<rte_eth_desc_lim>() - 2usize];
    ["Offset of field: rte_eth_desc_lim::nb_max"]
        [::core::mem::offset_of!(rte_eth_desc_lim, nb_max) - 0usize];
    ["Offset of field: rte_eth_desc_lim::nb_min"]
        [::core::mem::offset_of!(rte_eth_desc_lim, nb_min) - 2usize];
    ["Offset of field: rte_eth_desc_lim::nb_align"]
        [::core::mem::offset_of!(rte_eth_desc_lim, nb_align) - 4usize];
    ["Offset of field: rte_eth_desc_lim::nb_seg_max"]
        [::core::mem::offset_of!(rte_eth_desc_lim, nb_seg_max) - 6usize];
    ["Offset of field: rte_eth_desc_lim::nb_mtu_seg_max"]
        [::core::mem::offset_of!(rte_eth_desc_lim, nb_mtu_seg_max) - 8usize];
};
#[doc = " A structure describing a memzone, which is a contiguous portion of\n physical memory identified by a name."]
#[repr(C, packed)]
#[derive(Copy, Clone)]
pub struct rte_memzone {
    #[doc = "< Name of the memory zone."]
    pub name: [::core::ffi::c_char; 32usize],
    #[doc = "< Start IO address."]
    pub iova: rte_iova_t,
    pub __bindgen_anon_1: rte_memzone__bindgen_ty_1,
    #[doc = "< Length of the memzone."]
    pub len: usize,
    #[doc = "< The page size of underlying memory"]
    pub hugepage_sz: u64,
    #[doc = "< NUMA socket ID."]
    pub socket_id: i32,
    #[doc = "< Characteristics of this memzone."]
    pub flags: u32,
}
#[repr(C)]
#[derive(Copy, Clone)]
pub union rte_memzone__bindgen_ty_1 {
    #[doc = "< Start virtual address."]
    pub addr: *mut ::core::ffi::c_void,
    #[doc = "< Makes sure addr is always 64-bits"]
    pub addr_64: u64,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_memzone__bindgen_ty_1"]
        [::core::mem::size_of::<rte_memzone__bindgen_ty_1>() - 8usize];
    ["Alignment of rte_memzone__bindgen_ty_1"]
        [::core::mem::align_of::<rte_memzone__bindgen_ty_1>() - 8usize];
    ["Offset of field: rte_memzone__bindgen_ty_1::addr"]
        [::core::mem::offset_of!(rte_memzone__bindgen_ty_1, addr) - 0usize];
    ["Offset of field: rte_memzone__bindgen_ty_1::addr_64"]
        [::core::mem::offset_of!(rte_memzone__bindgen_ty_1, addr_64) - 0usize];
};
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_memzone"][::core::mem::size_of::<rte_memzone>() - 72usize];
    ["Alignment of rte_memzone"][::core::mem::align_of::<rte_memzone>() - 1usize];
    ["Offset of field: rte_memzone::name"][::core::mem::offset_of!(rte_memzone, name) - 0usize];
    ["Offset of field: rte_memzone::iova"][::core::mem::offset_of!(rte_memzone, iova) - 32usize];
    ["Offset of field: rte_memzone::len"][::core::mem::offset_of!(rte_memzone, len) - 48usize];
    ["Offset of field: rte_memzone::hugepage_sz"]
        [::core::mem::offset_of!(rte_memzone, hugepage_sz) - 56usize];
    ["Offset of field: rte_memzone::socket_id"]
        [::core::mem::offset_of!(rte_memzone, socket_id) - 64usize];
    ["Offset of field: rte_memzone::flags"][::core::mem::offset_of!(rte_memzone, flags) - 68usize];
};
#[doc = " A structure that stores a per-core object cache."]
#[repr(C)]
#[repr(align(64))]
#[derive(Debug, Copy, Clone)]
pub struct rte_mempool_cache {
    #[doc = "< Size of the cache"]
    pub size: u32,
    #[doc = "< Threshold before we flush excess elements"]
    pub flushthresh: u32,
    #[doc = "< Current cache count"]
    pub len: u32,
    pub __bindgen_padding_0: [u64; 6usize],
    #[doc = " Cache objects\n\n Cache is allocated to this size to allow it to overflow in certain\n cases to avoid needless emptying of cache."]
    pub objs: [*mut ::core::ffi::c_void; 1024usize],
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mempool_cache"][::core::mem::size_of::<rte_mempool_cache>() - 8256usize];
    ["Alignment of rte_mempool_cache"][::core::mem::align_of::<rte_mempool_cache>() - 64usize];
    ["Offset of field: rte_mempool_cache::size"]
        [::core::mem::offset_of!(rte_mempool_cache, size) - 0usize];
    ["Offset of field: rte_mempool_cache::flushthresh"]
        [::core::mem::offset_of!(rte_mempool_cache, flushthresh) - 4usize];
    ["Offset of field: rte_mempool_cache::len"]
        [::core::mem::offset_of!(rte_mempool_cache, len) - 8usize];
    ["Offset of field: rte_mempool_cache::objs"]
        [::core::mem::offset_of!(rte_mempool_cache, objs) - 64usize];
};
#[doc = " Mempool object header structure\n\n Each object stored in mempools are prefixed by this header structure,\n it allows to retrieve the mempool pointer from the object and to\n iterate on all objects attached to a mempool. When debug is enabled,\n a cookie is also added in this structure preventing corruptions and\n double-frees."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_mempool_objhdr {
    #[doc = "< Next in list."]
    pub next: rte_mempool_objhdr__bindgen_ty_1,
    #[doc = "< The mempool owning the object."]
    pub mp: *mut rte_mempool,
    #[doc = "< IO address of the object."]
    pub iova: rte_iova_t,
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_mempool_objhdr__bindgen_ty_1 {
    pub stqe_next: *mut rte_mempool_objhdr,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mempool_objhdr__bindgen_ty_1"]
        [::core::mem::size_of::<rte_mempool_objhdr__bindgen_ty_1>() - 8usize];
    ["Alignment of rte_mempool_objhdr__bindgen_ty_1"]
        [::core::mem::align_of::<rte_mempool_objhdr__bindgen_ty_1>() - 8usize];
    ["Offset of field: rte_mempool_objhdr__bindgen_ty_1::stqe_next"]
        [::core::mem::offset_of!(rte_mempool_objhdr__bindgen_ty_1, stqe_next) - 0usize];
};
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mempool_objhdr"][::core::mem::size_of::<rte_mempool_objhdr>() - 24usize];
    ["Alignment of rte_mempool_objhdr"][::core::mem::align_of::<rte_mempool_objhdr>() - 8usize];
    ["Offset of field: rte_mempool_objhdr::next"]
        [::core::mem::offset_of!(rte_mempool_objhdr, next) - 0usize];
    ["Offset of field: rte_mempool_objhdr::mp"]
        [::core::mem::offset_of!(rte_mempool_objhdr, mp) - 8usize];
    ["Offset of field: rte_mempool_objhdr::iova"]
        [::core::mem::offset_of!(rte_mempool_objhdr, iova) - 16usize];
};
#[doc = " A list of object headers type"]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_mempool_objhdr_list {
    pub stqh_first: *mut rte_mempool_objhdr,
    pub stqh_last: *mut *mut rte_mempool_objhdr,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mempool_objhdr_list"]
        [::core::mem::size_of::<rte_mempool_objhdr_list>() - 16usize];
    ["Alignment of rte_mempool_objhdr_list"]
        [::core::mem::align_of::<rte_mempool_objhdr_list>() - 8usize];
    ["Offset of field: rte_mempool_objhdr_list::stqh_first"]
        [::core::mem::offset_of!(rte_mempool_objhdr_list, stqh_first) - 0usize];
    ["Offset of field: rte_mempool_objhdr_list::stqh_last"]
        [::core::mem::offset_of!(rte_mempool_objhdr_list, stqh_last) - 8usize];
};
#[doc = " A list of memory where objects are stored"]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_mempool_memhdr_list {
    pub stqh_first: *mut rte_mempool_memhdr,
    pub stqh_last: *mut *mut rte_mempool_memhdr,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mempool_memhdr_list"]
        [::core::mem::size_of::<rte_mempool_memhdr_list>() - 16usize];
    ["Alignment of rte_mempool_memhdr_list"]
        [::core::mem::align_of::<rte_mempool_memhdr_list>() - 8usize];
    ["Offset of field: rte_mempool_memhdr_list::stqh_first"]
        [::core::mem::offset_of!(rte_mempool_memhdr_list, stqh_first) - 0usize];
    ["Offset of field: rte_mempool_memhdr_list::stqh_last"]
        [::core::mem::offset_of!(rte_mempool_memhdr_list, stqh_last) - 8usize];
};
#[doc = " Callback used to free a memory chunk"]
pub type rte_mempool_memchunk_free_cb_t = ::core::option::Option<
    unsafe extern "C" fn(memhdr: *mut rte_mempool_memhdr, opaque: *mut ::core::ffi::c_void),
>;
#[doc = " Mempool objects memory header structure\n\n The memory chunks where objects are stored. Each chunk is virtually\n and physically contiguous."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_mempool_memhdr {
    #[doc = "< Next in list."]
    pub next: rte_mempool_memhdr__bindgen_ty_1,
    #[doc = "< The mempool owning the chunk"]
    pub mp: *mut rte_mempool,
    #[doc = "< Virtual address of the chunk"]
    pub addr: *mut ::core::ffi::c_void,
    #[doc = "< IO address of the chunk"]
    pub iova: rte_iova_t,
    #[doc = "< length of the chunk"]
    pub len: usize,
    #[doc = "< Free callback"]
    pub free_cb: rte_mempool_memchunk_free_cb_t,
    #[doc = "< Argument passed to the free callback"]
    pub opaque: *mut ::core::ffi::c_void,
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_mempool_memhdr__bindgen_ty_1 {
    pub stqe_next: *mut rte_mempool_memhdr,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mempool_memhdr__bindgen_ty_1"]
        [::core::mem::size_of::<rte_mempool_memhdr__bindgen_ty_1>() - 8usize];
    ["Alignment of rte_mempool_memhdr__bindgen_ty_1"]
        [::core::mem::align_of::<rte_mempool_memhdr__bindgen_ty_1>() - 8usize];
    ["Offset of field: rte_mempool_memhdr__bindgen_ty_1::stqe_next"]
        [::core::mem::offset_of!(rte_mempool_memhdr__bindgen_ty_1, stqe_next) - 0usize];
};
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mempool_memhdr"][::core::mem::size_of::<rte_mempool_memhdr>() - 56usize];
    ["Alignment of rte_mempool_memhdr"][::core::mem::align_of::<rte_mempool_memhdr>() - 8usize];
    ["Offset of field: rte_mempool_memhdr::next"]
        [::core::mem::offset_of!(rte_mempool_memhdr, next) - 0usize];
    ["Offset of field: rte_mempool_memhdr::mp"]
        [::core::mem::offset_of!(rte_mempool_memhdr, mp) - 8usize];
    ["Offset of field: rte_mempool_memhdr::addr"]
        [::core::mem::offset_of!(rte_mempool_memhdr, addr) - 16usize];
    ["Offset of field: rte_mempool_memhdr::iova"]
        [::core::mem::offset_of!(rte_mempool_memhdr, iova) - 24usize];
    ["Offset of field: rte_mempool_memhdr::len"]
        [::core::mem::offset_of!(rte_mempool_memhdr, len) - 32usize];
    ["Offset of field: rte_mempool_memhdr::free_cb"]
        [::core::mem::offset_of!(rte_mempool_memhdr, free_cb) - 40usize];
    ["Offset of field: rte_mempool_memhdr::opaque"]
        [::core::mem::offset_of!(rte_mempool_memhdr, opaque) - 48usize];
};
#[doc = " The RTE mempool structure."]
#[repr(C)]
#[repr(align(64))]
#[derive(Copy, Clone)]
pub struct rte_mempool {
    #[doc = "< Name of mempool."]
    pub name: [::core::ffi::c_char; 26usize],
    pub __bindgen_anon_1: rte_mempool__bindgen_ty_1,
    #[doc = "< optional args for ops alloc."]
    pub pool_config: *mut ::core::ffi::c_void,
    #[doc = "< Memzone where pool is alloc'd."]
    pub mz: *const rte_memzone,
    #[doc = "< Flags of the mempool."]
    pub flags: ::core::ffi::c_uint,
    #[doc = "< Socket id passed at create."]
    pub socket_id: ::core::ffi::c_int,
    #[doc = "< Max size of the mempool."]
    pub size: u32,
    pub cache_size: u32,
    #[doc = "< Size of an element."]
    pub elt_size: u32,
    #[doc = "< Size of header (before elt)."]
    pub header_size: u32,
    #[doc = "< Size of trailer (after elt)."]
    pub trailer_size: u32,
    #[doc = "< Size of private data."]
    pub private_data_size: ::core::ffi::c_uint,
    #[doc = " Index into rte_mempool_ops_table array of mempool ops\n structs, which contain callback function pointers.\n We're using an index here rather than pointers to the callbacks\n to facilitate any secondary processes that may want to use\n this mempool."]
    pub ops_index: i32,
    #[doc = "< Per-lcore local cache"]
    pub local_cache: *mut rte_mempool_cache,
    #[doc = "< Number of populated objects."]
    pub populated_size: u32,
    #[doc = "< List of objects in pool"]
    pub elt_list: rte_mempool_objhdr_list,
    #[doc = "< Number of memory chunks"]
    pub nb_mem_chunks: u32,
    #[doc = "< List of memory chunks"]
    pub mem_list: rte_mempool_memhdr_list,
}
#[repr(C)]
#[derive(Copy, Clone)]
pub union rte_mempool__bindgen_ty_1 {
    #[doc = "< Ring or pool to store objects."]
    pub pool_data: *mut ::core::ffi::c_void,
    #[doc = "< External mempool identifier."]
    pub pool_id: u64,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mempool__bindgen_ty_1"]
        [::core::mem::size_of::<rte_mempool__bindgen_ty_1>() - 8usize];
    ["Alignment of rte_mempool__bindgen_ty_1"]
        [::core::mem::align_of::<rte_mempool__bindgen_ty_1>() - 8usize];
    ["Offset of field: rte_mempool__bindgen_ty_1::pool_data"]
        [::core::mem::offset_of!(rte_mempool__bindgen_ty_1, pool_data) - 0usize];
    ["Offset of field: rte_mempool__bindgen_ty_1::pool_id"]
        [::core::mem::offset_of!(rte_mempool__bindgen_ty_1, pool_id) - 0usize];
};
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mempool"][::core::mem::size_of::<rte_mempool>() - 192usize];
    ["Alignment of rte_mempool"][::core::mem::align_of::<rte_mempool>() - 64usize];
    ["Offset of field: rte_mempool::name"][::core::mem::offset_of!(rte_mempool, name) - 0usize];
    ["Offset of field: rte_mempool::pool_config"]
        [::core::mem::offset_of!(rte_mempool, pool_config) - 40usize];
    ["Offset of field: rte_mempool::mz"][::core::mem::offset_of!(rte_mempool, mz) - 48usize];
    ["Offset of field: rte_mempool::flags"][::core::mem::offset_of!(rte_mempool, flags) - 56usize];
    ["Offset of field: rte_mempool::socket_id"]
        [::core::mem::offset_of!(rte_mempool, socket_id) - 60usize];
    ["Offset of field: rte_mempool::size"][::core::mem::offset_of!(rte_mempool, size) - 64usize];
    ["Offset of field: rte_mempool::cache_size"]
        [::core::mem::offset_of!(rte_mempool, cache_size) - 68usize];
    ["Offset of field: rte_mempool::elt_size"]
        [::core::mem::offset_of!(rte_mempool, elt_size) - 72usize];
    ["Offset of field: rte_mempool::header_size"]
        [::core::mem::offset_of!(rte_mempool, header_size) - 76usize];
    ["Offset of field: rte_mempool::trailer_size"]
        [::core::mem::offset_of!(rte_mempool, trailer_size) - 80usize];
    ["Offset of field: rte_mempool::private_data_size"]
        [::core::mem::offset_of!(rte_mempool, private_data_size) - 84usize];
    ["Offset of field: rte_mempool::ops_index"]
        [::core::mem::offset_of!(rte_mempool, ops_index) - 88usize];
    ["Offset of field: rte_mempool::local_cache"]
        [::core::mem::offset_of!(rte_mempool, local_cache) - 96usize];
    ["Offset of field: rte_mempool::populated_size"]
        [::core::mem::offset_of!(rte_mempool, populated_size) - 104usize];
    ["Offset of field: rte_mempool::elt_list"]
        [::core::mem::offset_of!(rte_mempool, elt_list) - 112usize];
    ["Offset of field: rte_mempool::nb_mem_chunks"]
        [::core::mem::offset_of!(rte_mempool, nb_mem_chunks) - 128usize];
    ["Offset of field: rte_mempool::mem_list"]
        [::core::mem::offset_of!(rte_mempool, mem_list) - 136usize];
};
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_mbuf_sched {
    #[doc = "< Queue ID."]
    pub queue_id: u32,
    pub traffic_class: u8,
    pub color: u8,
    #[doc = "< Reserved."]
    pub reserved: u16,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mbuf_sched"][::core::mem::size_of::<rte_mbuf_sched>() - 8usize];
    ["Alignment of rte_mbuf_sched"][::core::mem::align_of::<rte_mbuf_sched>() - 4usize];
    ["Offset of field: rte_mbuf_sched::queue_id"]
        [::core::mem::offset_of!(rte_mbuf_sched, queue_id) - 0usize];
    ["Offset of field: rte_mbuf_sched::traffic_class"]
        [::core::mem::offset_of!(rte_mbuf_sched, traffic_class) - 4usize];
    ["Offset of field: rte_mbuf_sched::color"]
        [::core::mem::offset_of!(rte_mbuf_sched, color) - 5usize];
    ["Offset of field: rte_mbuf_sched::reserved"]
        [::core::mem::offset_of!(rte_mbuf_sched, reserved) - 6usize];
};
#[doc = " The generic rte_mbuf, containing a packet mbuf."]
#[repr(C)]
#[repr(align(64))]
pub struct rte_mbuf {
    pub cacheline0: RTE_MARKER,
    #[doc = "< Virtual address of segment buffer."]
    pub buf_addr: *mut ::core::ffi::c_void,
    #[doc = " Physical address of segment buffer.\n This field is undefined if the build is configured to use only\n virtual address as IOVA (i.e. RTE_IOVA_IN_MBUF is 0).\n Force alignment to 8-bytes, so as to ensure we have the exact\n same mbuf cacheline0 layout for 32-bit and 64-bit. This makes\n working on vector drivers easier."]
    pub buf_iova: rte_iova_t,
    pub rearm_data: RTE_MARKER64,
    pub data_off: u16,
    #[doc = " Reference counter. Its size should at least equal to the size\n of port field (16 bits), to support zero-copy broadcast.\n It should only be accessed using the following functions:\n rte_mbuf_refcnt_update(), rte_mbuf_refcnt_read(), and\n rte_mbuf_refcnt_set(). The functionality of these functions (atomic,\n or non-atomic) is controlled by the RTE_MBUF_REFCNT_ATOMIC flag."]
    pub refcnt: u16,
    #[doc = " Number of segments. Only valid for the first segment of an mbuf\n chain."]
    pub nb_segs: u16,
    #[doc = " Input port (16 bits to support more than 256 virtual ports).\n The event eth Tx adapter uses this field to specify the output port."]
    pub port: u16,
    #[doc = "< Offload features."]
    pub ol_flags: u64,
    pub rx_descriptor_fields1: RTE_MARKER,
    pub __bindgen_anon_1: rte_mbuf__bindgen_ty_1,
    #[doc = "< Total pkt len: sum of all segments."]
    pub pkt_len: u32,
    #[doc = "< Amount of data in segment buffer."]
    pub data_len: u16,
    #[doc = " VLAN TCI (CPU order), valid if RTE_MBUF_F_RX_VLAN is set."]
    pub vlan_tci: u16,
    pub __bindgen_anon_2: rte_mbuf__bindgen_ty_2,
    #[doc = " Outer VLAN TCI (CPU order), valid if RTE_MBUF_F_RX_QINQ is set."]
    pub vlan_tci_outer: u16,
    #[doc = "< Length of segment buffer."]
    pub buf_len: u16,
    #[doc = "< Pool from which mbuf was allocated."]
    pub pool: *mut rte_mempool,
    pub cacheline1: RTE_MARKER,
    #[doc = " Next segment of scattered packet. Must be NULL in the last\n segment or in case of non-segmented packet."]
    pub next: *mut rte_mbuf,
    pub __bindgen_anon_3: rte_mbuf__bindgen_ty_3,
    #[doc = " Shared data for external buffer attached to mbuf. See\n rte_pktmbuf_attach_extbuf()."]
    pub shinfo: *mut rte_mbuf_ext_shared_info,
    #[doc = " Size of the application private data. In case of an indirect\n mbuf, it stores the direct mbuf private data size."]
    pub priv_size: u16,
    #[doc = " Timesync flags for use with IEEE1588."]
    pub timesync: u16,
    #[doc = "< Reserved for dynamic fields."]
    pub dynfield1: [u32; 9usize],
}
#[repr(C)]
#[derive(Copy, Clone)]
pub union rte_mbuf__bindgen_ty_1 {
    #[doc = "< L2/L3/L4 and tunnel information."]
    pub packet_type: u32,
    pub __bindgen_anon_1: rte_mbuf__bindgen_ty_1__bindgen_ty_1,
}
#[repr(C)]
#[derive(Copy, Clone)]
pub struct rte_mbuf__bindgen_ty_1__bindgen_ty_1 {
    pub _bitfield_align_1: [u8; 0],
    pub _bitfield_1: __BindgenBitfieldUnit<[u8; 2usize]>,
    pub __bindgen_anon_1: rte_mbuf__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1,
    pub _bitfield_align_2: [u8; 0],
    pub _bitfield_2: __BindgenBitfieldUnit<[u8; 1usize]>,
}
#[repr(C)]
#[derive(Copy, Clone)]
pub union rte_mbuf__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1 {
    pub inner_esp_next_proto: u8,
    pub __bindgen_anon_1: rte_mbuf__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1,
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_mbuf__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1 {
    pub _bitfield_align_1: [u8; 0],
    pub _bitfield_1: __BindgenBitfieldUnit<[u8; 1usize]>,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mbuf__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1"]
        [::core::mem::size_of::<rte_mbuf__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1>(
        ) - 1usize];
    ["Alignment of rte_mbuf__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1"]
        [::core::mem::align_of::<rte_mbuf__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1>(
        ) - 1usize];
};
impl rte_mbuf__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1 {
    #[inline]
    pub fn inner_l2_type(&self) -> u8 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(0usize, 4u8) as u8) }
    }
    #[inline]
    pub fn set_inner_l2_type(&mut self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            self._bitfield_1.set(0usize, 4u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn inner_l2_type_raw(this: *const Self) -> u8 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 1usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                0usize,
                4u8,
            ) as u8)
        }
    }
    #[inline]
    pub unsafe fn set_inner_l2_type_raw(this: *mut Self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 1usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                0usize,
                4u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn inner_l3_type(&self) -> u8 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(4usize, 4u8) as u8) }
    }
    #[inline]
    pub fn set_inner_l3_type(&mut self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            self._bitfield_1.set(4usize, 4u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn inner_l3_type_raw(this: *const Self) -> u8 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 1usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                4usize,
                4u8,
            ) as u8)
        }
    }
    #[inline]
    pub unsafe fn set_inner_l3_type_raw(this: *mut Self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 1usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                4usize,
                4u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn new_bitfield_1(
        inner_l2_type: u8,
        inner_l3_type: u8,
    ) -> __BindgenBitfieldUnit<[u8; 1usize]> {
        let mut __bindgen_bitfield_unit: __BindgenBitfieldUnit<[u8; 1usize]> = Default::default();
        __bindgen_bitfield_unit.set(0usize, 4u8, {
            let inner_l2_type: u8 = unsafe { ::core::mem::transmute(inner_l2_type) };
            inner_l2_type as u64
        });
        __bindgen_bitfield_unit.set(4usize, 4u8, {
            let inner_l3_type: u8 = unsafe { ::core::mem::transmute(inner_l3_type) };
            inner_l3_type as u64
        });
        __bindgen_bitfield_unit
    }
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mbuf__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1"]
        [::core::mem::size_of::<rte_mbuf__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1>() - 1usize];
    ["Alignment of rte_mbuf__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1"]
        [::core::mem::align_of::<rte_mbuf__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1>() - 1usize];
    ["Offset of field: rte_mbuf__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1::inner_esp_next_proto"][::core::mem::offset_of!(
        rte_mbuf__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1,
        inner_esp_next_proto
    )
        - 0usize];
};
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mbuf__bindgen_ty_1__bindgen_ty_1"]
        [::core::mem::size_of::<rte_mbuf__bindgen_ty_1__bindgen_ty_1>() - 4usize];
    ["Alignment of rte_mbuf__bindgen_ty_1__bindgen_ty_1"]
        [::core::mem::align_of::<rte_mbuf__bindgen_ty_1__bindgen_ty_1>() - 1usize];
};
impl rte_mbuf__bindgen_ty_1__bindgen_ty_1 {
    #[inline]
    pub fn l2_type(&self) -> u8 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(0usize, 4u8) as u8) }
    }
    #[inline]
    pub fn set_l2_type(&mut self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            self._bitfield_1.set(0usize, 4u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn l2_type_raw(this: *const Self) -> u8 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 2usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                0usize,
                4u8,
            ) as u8)
        }
    }
    #[inline]
    pub unsafe fn set_l2_type_raw(this: *mut Self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 2usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                0usize,
                4u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn l3_type(&self) -> u8 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(4usize, 4u8) as u8) }
    }
    #[inline]
    pub fn set_l3_type(&mut self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            self._bitfield_1.set(4usize, 4u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn l3_type_raw(this: *const Self) -> u8 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 2usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                4usize,
                4u8,
            ) as u8)
        }
    }
    #[inline]
    pub unsafe fn set_l3_type_raw(this: *mut Self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 2usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                4usize,
                4u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn l4_type(&self) -> u8 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(8usize, 4u8) as u8) }
    }
    #[inline]
    pub fn set_l4_type(&mut self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            self._bitfield_1.set(8usize, 4u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn l4_type_raw(this: *const Self) -> u8 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 2usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                8usize,
                4u8,
            ) as u8)
        }
    }
    #[inline]
    pub unsafe fn set_l4_type_raw(this: *mut Self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 2usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                8usize,
                4u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn tun_type(&self) -> u8 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(12usize, 4u8) as u8) }
    }
    #[inline]
    pub fn set_tun_type(&mut self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            self._bitfield_1.set(12usize, 4u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn tun_type_raw(this: *const Self) -> u8 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 2usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                12usize,
                4u8,
            ) as u8)
        }
    }
    #[inline]
    pub unsafe fn set_tun_type_raw(this: *mut Self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 2usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                12usize,
                4u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn new_bitfield_1(
        l2_type: u8,
        l3_type: u8,
        l4_type: u8,
        tun_type: u8,
    ) -> __BindgenBitfieldUnit<[u8; 2usize]> {
        let mut __bindgen_bitfield_unit: __BindgenBitfieldUnit<[u8; 2usize]> = Default::default();
        __bindgen_bitfield_unit.set(0usize, 4u8, {
            let l2_type: u8 = unsafe { ::core::mem::transmute(l2_type) };
            l2_type as u64
        });
        __bindgen_bitfield_unit.set(4usize, 4u8, {
            let l3_type: u8 = unsafe { ::core::mem::transmute(l3_type) };
            l3_type as u64
        });
        __bindgen_bitfield_unit.set(8usize, 4u8, {
            let l4_type: u8 = unsafe { ::core::mem::transmute(l4_type) };
            l4_type as u64
        });
        __bindgen_bitfield_unit.set(12usize, 4u8, {
            let tun_type: u8 = unsafe { ::core::mem::transmute(tun_type) };
            tun_type as u64
        });
        __bindgen_bitfield_unit
    }
    #[inline]
    pub fn inner_l4_type(&self) -> u8 {
        unsafe { ::core::mem::transmute(self._bitfield_2.get(0usize, 4u8) as u8) }
    }
    #[inline]
    pub fn set_inner_l4_type(&mut self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            self._bitfield_2.set(0usize, 4u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn inner_l4_type_raw(this: *const Self) -> u8 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 1usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_2),
                0usize,
                4u8,
            ) as u8)
        }
    }
    #[inline]
    pub unsafe fn set_inner_l4_type_raw(this: *mut Self, val: u8) {
        unsafe {
            let val: u8 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 1usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_2),
                0usize,
                4u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn new_bitfield_2(inner_l4_type: u8) -> __BindgenBitfieldUnit<[u8; 1usize]> {
        let mut __bindgen_bitfield_unit: __BindgenBitfieldUnit<[u8; 1usize]> = Default::default();
        __bindgen_bitfield_unit.set(0usize, 4u8, {
            let inner_l4_type: u8 = unsafe { ::core::mem::transmute(inner_l4_type) };
            inner_l4_type as u64
        });
        __bindgen_bitfield_unit
    }
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mbuf__bindgen_ty_1"][::core::mem::size_of::<rte_mbuf__bindgen_ty_1>() - 4usize];
    ["Alignment of rte_mbuf__bindgen_ty_1"]
        [::core::mem::align_of::<rte_mbuf__bindgen_ty_1>() - 4usize];
    ["Offset of field: rte_mbuf__bindgen_ty_1::packet_type"]
        [::core::mem::offset_of!(rte_mbuf__bindgen_ty_1, packet_type) - 0usize];
};
#[repr(C)]
#[derive(Copy, Clone)]
pub union rte_mbuf__bindgen_ty_2 {
    #[doc = "< hash information"]
    pub hash: rte_mbuf__bindgen_ty_2__bindgen_ty_1,
}
#[repr(C)]
#[derive(Copy, Clone)]
pub union rte_mbuf__bindgen_ty_2__bindgen_ty_1 {
    #[doc = "< RSS hash result if RSS enabled"]
    pub rss: u32,
    #[doc = "< Filter identifier if FDIR enabled"]
    pub fdir: rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1,
    pub sched: rte_mbuf_sched,
    #[doc = "< Eventdev ethdev Tx adapter"]
    pub txadapter: rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_2,
    pub usr: u32,
}
#[repr(C)]
#[derive(Copy, Clone)]
pub struct rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1 {
    pub __bindgen_anon_1: rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1,
    pub hi: u32,
}
#[repr(C)]
#[derive(Copy, Clone)]
pub union rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1 {
    pub __bindgen_anon_1:
        rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1,
    pub lo: u32,
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1 {
    pub hash: u16,
    pub id: u16,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1"]
        [::core::mem::size_of::<
            rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1,
        >() - 4usize];
    ["Alignment of rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1"] [:: core :: mem :: align_of :: < rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1 > () - 2usize] ;
    ["Offset of field: rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1::hash"] [:: core :: mem :: offset_of ! (rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1 , hash) - 0usize] ;
    ["Offset of field: rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1::id"] [:: core :: mem :: offset_of ! (rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1 , id) - 2usize] ;
};
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1"]
        [::core::mem::size_of::<rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1>(
        ) - 4usize];
    ["Alignment of rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1"]
        [::core::mem::align_of::<rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1>(
        ) - 4usize];
    ["Offset of field: rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1::lo"][::core::mem::offset_of!(
        rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1__bindgen_ty_1,
        lo
    )
        - 0usize];
};
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1"]
        [::core::mem::size_of::<rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1>() - 8usize];
    ["Alignment of rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1"]
        [::core::mem::align_of::<rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1>() - 4usize];
    ["Offset of field: rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1::hi"]
        [::core::mem::offset_of!(rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_1, hi) - 4usize];
};
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_2 {
    pub reserved1: u32,
    pub reserved2: u16,
    pub txq: u16,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_2"]
        [::core::mem::size_of::<rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_2>() - 8usize];
    ["Alignment of rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_2"]
        [::core::mem::align_of::<rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_2>() - 4usize];
    ["Offset of field: rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_2::reserved1"][::core::mem::offset_of!(
        rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_2,
        reserved1
    ) - 0usize];
    ["Offset of field: rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_2::reserved2"][::core::mem::offset_of!(
        rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_2,
        reserved2
    ) - 4usize];
    ["Offset of field: rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_2::txq"]
        [::core::mem::offset_of!(rte_mbuf__bindgen_ty_2__bindgen_ty_1__bindgen_ty_2, txq) - 6usize];
};
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mbuf__bindgen_ty_2__bindgen_ty_1"]
        [::core::mem::size_of::<rte_mbuf__bindgen_ty_2__bindgen_ty_1>() - 8usize];
    ["Alignment of rte_mbuf__bindgen_ty_2__bindgen_ty_1"]
        [::core::mem::align_of::<rte_mbuf__bindgen_ty_2__bindgen_ty_1>() - 4usize];
    ["Offset of field: rte_mbuf__bindgen_ty_2__bindgen_ty_1::rss"]
        [::core::mem::offset_of!(rte_mbuf__bindgen_ty_2__bindgen_ty_1, rss) - 0usize];
    ["Offset of field: rte_mbuf__bindgen_ty_2__bindgen_ty_1::fdir"]
        [::core::mem::offset_of!(rte_mbuf__bindgen_ty_2__bindgen_ty_1, fdir) - 0usize];
    ["Offset of field: rte_mbuf__bindgen_ty_2__bindgen_ty_1::sched"]
        [::core::mem::offset_of!(rte_mbuf__bindgen_ty_2__bindgen_ty_1, sched) - 0usize];
    ["Offset of field: rte_mbuf__bindgen_ty_2__bindgen_ty_1::txadapter"]
        [::core::mem::offset_of!(rte_mbuf__bindgen_ty_2__bindgen_ty_1, txadapter) - 0usize];
    ["Offset of field: rte_mbuf__bindgen_ty_2__bindgen_ty_1::usr"]
        [::core::mem::offset_of!(rte_mbuf__bindgen_ty_2__bindgen_ty_1, usr) - 0usize];
};
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mbuf__bindgen_ty_2"][::core::mem::size_of::<rte_mbuf__bindgen_ty_2>() - 8usize];
    ["Alignment of rte_mbuf__bindgen_ty_2"]
        [::core::mem::align_of::<rte_mbuf__bindgen_ty_2>() - 4usize];
    ["Offset of field: rte_mbuf__bindgen_ty_2::hash"]
        [::core::mem::offset_of!(rte_mbuf__bindgen_ty_2, hash) - 0usize];
};
#[repr(C)]
#[derive(Copy, Clone)]
pub union rte_mbuf__bindgen_ty_3 {
    #[doc = "< combined for easy fetch"]
    pub tx_offload: u64,
    pub __bindgen_anon_1: rte_mbuf__bindgen_ty_3__bindgen_ty_1,
}
#[repr(C)]
#[repr(align(8))]
#[derive(Debug, Copy, Clone)]
pub struct rte_mbuf__bindgen_ty_3__bindgen_ty_1 {
    pub _bitfield_align_1: [u16; 0],
    pub _bitfield_1: __BindgenBitfieldUnit<[u8; 7usize]>,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mbuf__bindgen_ty_3__bindgen_ty_1"]
        [::core::mem::size_of::<rte_mbuf__bindgen_ty_3__bindgen_ty_1>() - 8usize];
    ["Alignment of rte_mbuf__bindgen_ty_3__bindgen_ty_1"]
        [::core::mem::align_of::<rte_mbuf__bindgen_ty_3__bindgen_ty_1>() - 8usize];
};
impl rte_mbuf__bindgen_ty_3__bindgen_ty_1 {
    #[inline]
    pub fn l2_len(&self) -> u64 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(0usize, 7u8) as u64) }
    }
    #[inline]
    pub fn set_l2_len(&mut self, val: u64) {
        unsafe {
            let val: u64 = ::core::mem::transmute(val);
            self._bitfield_1.set(0usize, 7u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn l2_len_raw(this: *const Self) -> u64 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 7usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                0usize,
                7u8,
            ) as u64)
        }
    }
    #[inline]
    pub unsafe fn set_l2_len_raw(this: *mut Self, val: u64) {
        unsafe {
            let val: u64 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 7usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                0usize,
                7u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn l3_len(&self) -> u64 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(7usize, 9u8) as u64) }
    }
    #[inline]
    pub fn set_l3_len(&mut self, val: u64) {
        unsafe {
            let val: u64 = ::core::mem::transmute(val);
            self._bitfield_1.set(7usize, 9u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn l3_len_raw(this: *const Self) -> u64 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 7usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                7usize,
                9u8,
            ) as u64)
        }
    }
    #[inline]
    pub unsafe fn set_l3_len_raw(this: *mut Self, val: u64) {
        unsafe {
            let val: u64 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 7usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                7usize,
                9u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn l4_len(&self) -> u64 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(16usize, 8u8) as u64) }
    }
    #[inline]
    pub fn set_l4_len(&mut self, val: u64) {
        unsafe {
            let val: u64 = ::core::mem::transmute(val);
            self._bitfield_1.set(16usize, 8u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn l4_len_raw(this: *const Self) -> u64 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 7usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                16usize,
                8u8,
            ) as u64)
        }
    }
    #[inline]
    pub unsafe fn set_l4_len_raw(this: *mut Self, val: u64) {
        unsafe {
            let val: u64 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 7usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                16usize,
                8u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn tso_segsz(&self) -> u64 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(24usize, 16u8) as u64) }
    }
    #[inline]
    pub fn set_tso_segsz(&mut self, val: u64) {
        unsafe {
            let val: u64 = ::core::mem::transmute(val);
            self._bitfield_1.set(24usize, 16u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn tso_segsz_raw(this: *const Self) -> u64 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 7usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                24usize,
                16u8,
            ) as u64)
        }
    }
    #[inline]
    pub unsafe fn set_tso_segsz_raw(this: *mut Self, val: u64) {
        unsafe {
            let val: u64 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 7usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                24usize,
                16u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn outer_l3_len(&self) -> u64 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(40usize, 9u8) as u64) }
    }
    #[inline]
    pub fn set_outer_l3_len(&mut self, val: u64) {
        unsafe {
            let val: u64 = ::core::mem::transmute(val);
            self._bitfield_1.set(40usize, 9u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn outer_l3_len_raw(this: *const Self) -> u64 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 7usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                40usize,
                9u8,
            ) as u64)
        }
    }
    #[inline]
    pub unsafe fn set_outer_l3_len_raw(this: *mut Self, val: u64) {
        unsafe {
            let val: u64 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 7usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                40usize,
                9u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn outer_l2_len(&self) -> u64 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(49usize, 7u8) as u64) }
    }
    #[inline]
    pub fn set_outer_l2_len(&mut self, val: u64) {
        unsafe {
            let val: u64 = ::core::mem::transmute(val);
            self._bitfield_1.set(49usize, 7u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn outer_l2_len_raw(this: *const Self) -> u64 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 7usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                49usize,
                7u8,
            ) as u64)
        }
    }
    #[inline]
    pub unsafe fn set_outer_l2_len_raw(this: *mut Self, val: u64) {
        unsafe {
            let val: u64 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 7usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                49usize,
                7u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn new_bitfield_1(
        l2_len: u64,
        l3_len: u64,
        l4_len: u64,
        tso_segsz: u64,
        outer_l3_len: u64,
        outer_l2_len: u64,
    ) -> __BindgenBitfieldUnit<[u8; 7usize]> {
        let mut __bindgen_bitfield_unit: __BindgenBitfieldUnit<[u8; 7usize]> = Default::default();
        __bindgen_bitfield_unit.set(0usize, 7u8, {
            let l2_len: u64 = unsafe { ::core::mem::transmute(l2_len) };
            l2_len as u64
        });
        __bindgen_bitfield_unit.set(7usize, 9u8, {
            let l3_len: u64 = unsafe { ::core::mem::transmute(l3_len) };
            l3_len as u64
        });
        __bindgen_bitfield_unit.set(16usize, 8u8, {
            let l4_len: u64 = unsafe { ::core::mem::transmute(l4_len) };
            l4_len as u64
        });
        __bindgen_bitfield_unit.set(24usize, 16u8, {
            let tso_segsz: u64 = unsafe { ::core::mem::transmute(tso_segsz) };
            tso_segsz as u64
        });
        __bindgen_bitfield_unit.set(40usize, 9u8, {
            let outer_l3_len: u64 = unsafe { ::core::mem::transmute(outer_l3_len) };
            outer_l3_len as u64
        });
        __bindgen_bitfield_unit.set(49usize, 7u8, {
            let outer_l2_len: u64 = unsafe { ::core::mem::transmute(outer_l2_len) };
            outer_l2_len as u64
        });
        __bindgen_bitfield_unit
    }
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mbuf__bindgen_ty_3"][::core::mem::size_of::<rte_mbuf__bindgen_ty_3>() - 8usize];
    ["Alignment of rte_mbuf__bindgen_ty_3"]
        [::core::mem::align_of::<rte_mbuf__bindgen_ty_3>() - 8usize];
    ["Offset of field: rte_mbuf__bindgen_ty_3::tx_offload"]
        [::core::mem::offset_of!(rte_mbuf__bindgen_ty_3, tx_offload) - 0usize];
};
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mbuf"][::core::mem::size_of::<rte_mbuf>() - 128usize];
    ["Alignment of rte_mbuf"][::core::mem::align_of::<rte_mbuf>() - 64usize];
    ["Offset of field: rte_mbuf::cacheline0"]
        [::core::mem::offset_of!(rte_mbuf, cacheline0) - 0usize];
    ["Offset of field: rte_mbuf::buf_addr"][::core::mem::offset_of!(rte_mbuf, buf_addr) - 0usize];
    ["Offset of field: rte_mbuf::buf_iova"][::core::mem::offset_of!(rte_mbuf, buf_iova) - 8usize];
    ["Offset of field: rte_mbuf::rearm_data"]
        [::core::mem::offset_of!(rte_mbuf, rearm_data) - 16usize];
    ["Offset of field: rte_mbuf::data_off"][::core::mem::offset_of!(rte_mbuf, data_off) - 16usize];
    ["Offset of field: rte_mbuf::refcnt"][::core::mem::offset_of!(rte_mbuf, refcnt) - 18usize];
    ["Offset of field: rte_mbuf::nb_segs"][::core::mem::offset_of!(rte_mbuf, nb_segs) - 20usize];
    ["Offset of field: rte_mbuf::port"][::core::mem::offset_of!(rte_mbuf, port) - 22usize];
    ["Offset of field: rte_mbuf::ol_flags"][::core::mem::offset_of!(rte_mbuf, ol_flags) - 24usize];
    ["Offset of field: rte_mbuf::rx_descriptor_fields1"]
        [::core::mem::offset_of!(rte_mbuf, rx_descriptor_fields1) - 32usize];
    ["Offset of field: rte_mbuf::pkt_len"][::core::mem::offset_of!(rte_mbuf, pkt_len) - 36usize];
    ["Offset of field: rte_mbuf::data_len"][::core::mem::offset_of!(rte_mbuf, data_len) - 40usize];
    ["Offset of field: rte_mbuf::vlan_tci"][::core::mem::offset_of!(rte_mbuf, vlan_tci) - 42usize];
    ["Offset of field: rte_mbuf::vlan_tci_outer"]
        [::core::mem::offset_of!(rte_mbuf, vlan_tci_outer) - 52usize];
    ["Offset of field: rte_mbuf::buf_len"][::core::mem::offset_of!(rte_mbuf, buf_len) - 54usize];
    ["Offset of field: rte_mbuf::pool"][::core::mem::offset_of!(rte_mbuf, pool) - 56usize];
    ["Offset of field: rte_mbuf::cacheline1"]
        [::core::mem::offset_of!(rte_mbuf, cacheline1) - 64usize];
    ["Offset of field: rte_mbuf::next"][::core::mem::offset_of!(rte_mbuf, next) - 64usize];
    ["Offset of field: rte_mbuf::shinfo"][::core::mem::offset_of!(rte_mbuf, shinfo) - 80usize];
    ["Offset of field: rte_mbuf::priv_size"]
        [::core::mem::offset_of!(rte_mbuf, priv_size) - 88usize];
    ["Offset of field: rte_mbuf::timesync"][::core::mem::offset_of!(rte_mbuf, timesync) - 90usize];
    ["Offset of field: rte_mbuf::dynfield1"]
        [::core::mem::offset_of!(rte_mbuf, dynfield1) - 92usize];
};
#[doc = " Function typedef of callback to free externally attached buffer."]
pub type rte_mbuf_extbuf_free_callback_t = ::core::option::Option<
    unsafe extern "C" fn(addr: *mut ::core::ffi::c_void, opaque: *mut ::core::ffi::c_void),
>;
#[doc = " Shared data at the end of an external buffer."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_mbuf_ext_shared_info {
    #[doc = "< Free callback function"]
    pub free_cb: rte_mbuf_extbuf_free_callback_t,
    #[doc = "< Free callback argument"]
    pub fcb_opaque: *mut ::core::ffi::c_void,
    pub refcnt: u16,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_mbuf_ext_shared_info"]
        [::core::mem::size_of::<rte_mbuf_ext_shared_info>() - 24usize];
    ["Alignment of rte_mbuf_ext_shared_info"]
        [::core::mem::align_of::<rte_mbuf_ext_shared_info>() - 8usize];
    ["Offset of field: rte_mbuf_ext_shared_info::free_cb"]
        [::core::mem::offset_of!(rte_mbuf_ext_shared_info, free_cb) - 0usize];
    ["Offset of field: rte_mbuf_ext_shared_info::fcb_opaque"]
        [::core::mem::offset_of!(rte_mbuf_ext_shared_info, fcb_opaque) - 8usize];
    ["Offset of field: rte_mbuf_ext_shared_info::refcnt"]
        [::core::mem::offset_of!(rte_mbuf_ext_shared_info, refcnt) - 16usize];
};
unsafe extern "C" {
    #[doc = " Create a mbuf pool.\n\n This function creates and initializes a packet mbuf pool. It is\n a wrapper to rte_mempool functions.\n\n @param name\n   The name of the mbuf pool.\n @param n\n   The number of elements in the mbuf pool. The optimum size (in terms\n   of memory usage) for a mempool is when n is a power of two minus one:\n   n = (2^q - 1).\n @param cache_size\n   Size of the per-core object cache. See rte_mempool_create() for\n   details.\n @param priv_size\n   Size of application private are between the rte_mbuf structure\n   and the data buffer. This value must be aligned to RTE_MBUF_PRIV_ALIGN.\n @param data_room_size\n   Size of data buffer in each mbuf, including RTE_PKTMBUF_HEADROOM.\n @param socket_id\n   The socket identifier where the memory should be allocated. The\n   value can be *SOCKET_ID_ANY* if there is no NUMA constraint for the\n   reserved zone.\n @return\n   The pointer to the new allocated mempool, on success. NULL on error\n   with rte_errno set appropriately. Possible rte_errno values include:\n    - E_RTE_NO_CONFIG - function could not get pointer to rte_config structure\n    - EINVAL - cache size provided is too large, or priv_size is not aligned.\n    - ENOSPC - the maximum number of memzones has already been allocated\n    - EEXIST - a memzone with the same name already exists\n    - ENOMEM - no appropriate memory area found in which to create memzone"]
    pub fn rte_pktmbuf_pool_create(
        name: *const ::core::ffi::c_char,
        n: ::core::ffi::c_uint,
        cache_size: ::core::ffi::c_uint,
        priv_size: u16,
        data_room_size: u16,
        socket_id: ::core::ffi::c_int,
    ) -> *mut rte_mempool;
}
#[doc = " Ethernet address:\n A universally administered address is uniquely assigned to a device by its\n manufacturer. The first three octets (in transmission order) contain the\n Organizationally Unique Identifier (OUI). The following three (MAC-48 and\n EUI-48) octets are assigned by that organization with the only constraint\n of uniqueness.\n A locally administered address is assigned to a device by a network\n administrator and does not contain OUIs.\n See http://standards.ieee.org/regauth/groupmac/tutorial.html"]
#[repr(C)]
#[repr(align(2))]
#[derive(Debug, Copy, Clone)]
pub struct rte_ether_addr {
    #[doc = "< Addr bytes in tx order"]
    pub addr_bytes: [u8; 6usize],
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_ether_addr"][::core::mem::size_of::<rte_ether_addr>() - 6usize];
    ["Alignment of rte_ether_addr"][::core::mem::align_of::<rte_ether_addr>() - 2usize];
    ["Offset of field: rte_ether_addr::addr_bytes"]
        [::core::mem::offset_of!(rte_ether_addr, addr_bytes) - 0usize];
};
#[doc = " A structure used to enable/disable specific device interrupts."]
#[repr(C)]
#[repr(align(4))]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_intr_conf {
    pub _bitfield_align_1: [u8; 0],
    pub _bitfield_1: __BindgenBitfieldUnit<[u8; 1usize]>,
    pub __bindgen_padding_0: [u8; 3usize],
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_intr_conf"][::core::mem::size_of::<rte_eth_intr_conf>() - 4usize];
    ["Alignment of rte_eth_intr_conf"][::core::mem::align_of::<rte_eth_intr_conf>() - 4usize];
};
impl rte_eth_intr_conf {
    #[inline]
    pub fn lsc(&self) -> u32 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(0usize, 1u8) as u32) }
    }
    #[inline]
    pub fn set_lsc(&mut self, val: u32) {
        unsafe {
            let val: u32 = ::core::mem::transmute(val);
            self._bitfield_1.set(0usize, 1u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn lsc_raw(this: *const Self) -> u32 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 1usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                0usize,
                1u8,
            ) as u32)
        }
    }
    #[inline]
    pub unsafe fn set_lsc_raw(this: *mut Self, val: u32) {
        unsafe {
            let val: u32 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 1usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                0usize,
                1u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn rxq(&self) -> u32 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(1usize, 1u8) as u32) }
    }
    #[inline]
    pub fn set_rxq(&mut self, val: u32) {
        unsafe {
            let val: u32 = ::core::mem::transmute(val);
            self._bitfield_1.set(1usize, 1u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn rxq_raw(this: *const Self) -> u32 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 1usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                1usize,
                1u8,
            ) as u32)
        }
    }
    #[inline]
    pub unsafe fn set_rxq_raw(this: *mut Self, val: u32) {
        unsafe {
            let val: u32 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 1usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                1usize,
                1u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn rmv(&self) -> u32 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(2usize, 1u8) as u32) }
    }
    #[inline]
    pub fn set_rmv(&mut self, val: u32) {
        unsafe {
            let val: u32 = ::core::mem::transmute(val);
            self._bitfield_1.set(2usize, 1u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn rmv_raw(this: *const Self) -> u32 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 1usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                2usize,
                1u8,
            ) as u32)
        }
    }
    #[inline]
    pub unsafe fn set_rmv_raw(this: *mut Self, val: u32) {
        unsafe {
            let val: u32 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 1usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                2usize,
                1u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn new_bitfield_1(lsc: u32, rxq: u32, rmv: u32) -> __BindgenBitfieldUnit<[u8; 1usize]> {
        let mut __bindgen_bitfield_unit: __BindgenBitfieldUnit<[u8; 1usize]> = Default::default();
        __bindgen_bitfield_unit.set(0usize, 1u8, {
            let lsc: u32 = unsafe { ::core::mem::transmute(lsc) };
            lsc as u64
        });
        __bindgen_bitfield_unit.set(1usize, 1u8, {
            let rxq: u32 = unsafe { ::core::mem::transmute(rxq) };
            rxq as u64
        });
        __bindgen_bitfield_unit.set(2usize, 1u8, {
            let rmv: u32 = unsafe { ::core::mem::transmute(rmv) };
            rmv as u64
        });
        __bindgen_bitfield_unit
    }
}
#[doc = " A structure used to configure an Ethernet port.\n Depending upon the Rx multi-queue mode, extra advanced\n configuration settings may be needed."]
#[repr(C)]
#[derive(Copy, Clone)]
pub struct rte_eth_conf {
    #[doc = "< bitmap of RTE_ETH_LINK_SPEED_XXX of speeds to be\nused. RTE_ETH_LINK_SPEED_FIXED disables link\nautonegotiation, and a unique speed shall be\nset. Otherwise, the bitmap defines the set of\nspeeds to be advertised. If the special value\nRTE_ETH_LINK_SPEED_AUTONEG (0) is used, all speeds\nsupported are advertised."]
    pub link_speeds: u32,
    #[doc = "< Port Rx configuration."]
    pub rxmode: rte_eth_rxmode,
    #[doc = "< Port Tx configuration."]
    pub txmode: rte_eth_txmode,
    #[doc = "< Loopback operation mode. By default the value\nis 0, meaning the loopback mode is disabled.\nRead the datasheet of given Ethernet controller\nfor details. The possible values of this field\nare defined in implementation of each driver."]
    pub lpbk_mode: u32,
    #[doc = "< Port Rx filtering configuration."]
    pub rx_adv_conf: rte_eth_conf__bindgen_ty_1,
    #[doc = "< Port Tx DCB configuration (union)."]
    pub tx_adv_conf: rte_eth_conf__bindgen_ty_2,
    #[doc = " Currently,Priority Flow Control(PFC) are supported,if DCB with PFC\nis needed,and the variable must be set RTE_ETH_DCB_PFC_SUPPORT."]
    pub dcb_capability_en: u32,
    #[doc = "< Interrupt mode configuration."]
    pub intr_conf: rte_eth_intr_conf,
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_conf__bindgen_ty_1 {
    #[doc = "< Port RSS configuration"]
    pub rss_conf: rte_eth_rss_conf,
    #[doc = " Port VMDq+DCB configuration."]
    pub vmdq_dcb_conf: rte_eth_vmdq_dcb_conf,
    #[doc = " Port DCB Rx configuration."]
    pub dcb_rx_conf: rte_eth_dcb_rx_conf,
    #[doc = " Port VMDq Rx configuration."]
    pub vmdq_rx_conf: rte_eth_vmdq_rx_conf,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_conf__bindgen_ty_1"]
        [::core::mem::size_of::<rte_eth_conf__bindgen_ty_1>() - 2128usize];
    ["Alignment of rte_eth_conf__bindgen_ty_1"]
        [::core::mem::align_of::<rte_eth_conf__bindgen_ty_1>() - 8usize];
    ["Offset of field: rte_eth_conf__bindgen_ty_1::rss_conf"]
        [::core::mem::offset_of!(rte_eth_conf__bindgen_ty_1, rss_conf) - 0usize];
    ["Offset of field: rte_eth_conf__bindgen_ty_1::vmdq_dcb_conf"]
        [::core::mem::offset_of!(rte_eth_conf__bindgen_ty_1, vmdq_dcb_conf) - 32usize];
    ["Offset of field: rte_eth_conf__bindgen_ty_1::dcb_rx_conf"]
        [::core::mem::offset_of!(rte_eth_conf__bindgen_ty_1, dcb_rx_conf) - 1072usize];
    ["Offset of field: rte_eth_conf__bindgen_ty_1::vmdq_rx_conf"]
        [::core::mem::offset_of!(rte_eth_conf__bindgen_ty_1, vmdq_rx_conf) - 1088usize];
};
#[repr(C)]
#[derive(Copy, Clone)]
pub union rte_eth_conf__bindgen_ty_2 {
    #[doc = " Port VMDq+DCB Tx configuration."]
    pub vmdq_dcb_tx_conf: rte_eth_vmdq_dcb_tx_conf,
    #[doc = " Port DCB Tx configuration."]
    pub dcb_tx_conf: rte_eth_dcb_tx_conf,
    #[doc = " Port VMDq Tx configuration."]
    pub vmdq_tx_conf: rte_eth_vmdq_tx_conf,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_conf__bindgen_ty_2"]
        [::core::mem::size_of::<rte_eth_conf__bindgen_ty_2>() - 12usize];
    ["Alignment of rte_eth_conf__bindgen_ty_2"]
        [::core::mem::align_of::<rte_eth_conf__bindgen_ty_2>() - 4usize];
    ["Offset of field: rte_eth_conf__bindgen_ty_2::vmdq_dcb_tx_conf"]
        [::core::mem::offset_of!(rte_eth_conf__bindgen_ty_2, vmdq_dcb_tx_conf) - 0usize];
    ["Offset of field: rte_eth_conf__bindgen_ty_2::dcb_tx_conf"]
        [::core::mem::offset_of!(rte_eth_conf__bindgen_ty_2, dcb_tx_conf) - 0usize];
    ["Offset of field: rte_eth_conf__bindgen_ty_2::vmdq_tx_conf"]
        [::core::mem::offset_of!(rte_eth_conf__bindgen_ty_2, vmdq_tx_conf) - 0usize];
};
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_conf"][::core::mem::size_of::<rte_eth_conf>() - 2280usize];
    ["Alignment of rte_eth_conf"][::core::mem::align_of::<rte_eth_conf>() - 8usize];
    ["Offset of field: rte_eth_conf::link_speeds"]
        [::core::mem::offset_of!(rte_eth_conf, link_speeds) - 0usize];
    ["Offset of field: rte_eth_conf::rxmode"]
        [::core::mem::offset_of!(rte_eth_conf, rxmode) - 8usize];
    ["Offset of field: rte_eth_conf::txmode"]
        [::core::mem::offset_of!(rte_eth_conf, txmode) - 64usize];
    ["Offset of field: rte_eth_conf::lpbk_mode"]
        [::core::mem::offset_of!(rte_eth_conf, lpbk_mode) - 120usize];
    ["Offset of field: rte_eth_conf::rx_adv_conf"]
        [::core::mem::offset_of!(rte_eth_conf, rx_adv_conf) - 128usize];
    ["Offset of field: rte_eth_conf::tx_adv_conf"]
        [::core::mem::offset_of!(rte_eth_conf, tx_adv_conf) - 2256usize];
    ["Offset of field: rte_eth_conf::dcb_capability_en"]
        [::core::mem::offset_of!(rte_eth_conf, dcb_capability_en) - 2268usize];
    ["Offset of field: rte_eth_conf::intr_conf"]
        [::core::mem::offset_of!(rte_eth_conf, intr_conf) - 2272usize];
};
#[doc = " Preferred Rx/Tx port parameters.\n There are separate instances of this structure for transmission\n and reception respectively."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_dev_portconf {
    #[doc = "< Device-preferred burst size"]
    pub burst_size: u16,
    #[doc = "< Device-preferred size of queue rings"]
    pub ring_size: u16,
    #[doc = "< Device-preferred number of queues"]
    pub nb_queues: u16,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_dev_portconf"][::core::mem::size_of::<rte_eth_dev_portconf>() - 6usize];
    ["Alignment of rte_eth_dev_portconf"][::core::mem::align_of::<rte_eth_dev_portconf>() - 2usize];
    ["Offset of field: rte_eth_dev_portconf::burst_size"]
        [::core::mem::offset_of!(rte_eth_dev_portconf, burst_size) - 0usize];
    ["Offset of field: rte_eth_dev_portconf::ring_size"]
        [::core::mem::offset_of!(rte_eth_dev_portconf, ring_size) - 2usize];
    ["Offset of field: rte_eth_dev_portconf::nb_queues"]
        [::core::mem::offset_of!(rte_eth_dev_portconf, nb_queues) - 4usize];
};
#[doc = " Ethernet device associated switch information"]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_switch_info {
    #[doc = "< switch name"]
    pub name: *const ::core::ffi::c_char,
    #[doc = "< switch domain ID"]
    pub domain_id: u16,
    #[doc = " Mapping to the devices physical switch port as enumerated from the\n perspective of the embedded interconnect/switch. For SR-IOV enabled\n device this may correspond to the VF_ID of each virtual function,\n but each driver should explicitly define the mapping of switch\n port identifier to that physical interconnect/switch"]
    pub port_id: u16,
    #[doc = " Shared Rx queue sub-domain boundary. Only ports in same Rx domain\n and switch domain can share Rx queue. Valid only if device advertised\n RTE_ETH_DEV_CAPA_RXQ_SHARE capability."]
    pub rx_domain: u16,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_switch_info"][::core::mem::size_of::<rte_eth_switch_info>() - 16usize];
    ["Alignment of rte_eth_switch_info"][::core::mem::align_of::<rte_eth_switch_info>() - 8usize];
    ["Offset of field: rte_eth_switch_info::name"]
        [::core::mem::offset_of!(rte_eth_switch_info, name) - 0usize];
    ["Offset of field: rte_eth_switch_info::domain_id"]
        [::core::mem::offset_of!(rte_eth_switch_info, domain_id) - 8usize];
    ["Offset of field: rte_eth_switch_info::port_id"]
        [::core::mem::offset_of!(rte_eth_switch_info, port_id) - 10usize];
    ["Offset of field: rte_eth_switch_info::rx_domain"]
        [::core::mem::offset_of!(rte_eth_switch_info, rx_domain) - 12usize];
};
#[doc = " @warning\n @b EXPERIMENTAL: this structure may change without prior notice.\n\n Ethernet device Rx buffer segmentation capabilities."]
#[repr(C)]
#[repr(align(4))]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_rxseg_capa {
    pub _bitfield_align_1: [u8; 0],
    pub _bitfield_1: __BindgenBitfieldUnit<[u8; 1usize]>,
    pub __bindgen_padding_0: u16,
    #[doc = "< Maximum amount of segments to split."]
    pub max_nseg: u16,
    #[doc = "< Reserved field."]
    pub reserved: u16,
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_rxseg_capa"][::core::mem::size_of::<rte_eth_rxseg_capa>() - 8usize];
    ["Alignment of rte_eth_rxseg_capa"][::core::mem::align_of::<rte_eth_rxseg_capa>() - 4usize];
    ["Offset of field: rte_eth_rxseg_capa::max_nseg"]
        [::core::mem::offset_of!(rte_eth_rxseg_capa, max_nseg) - 4usize];
    ["Offset of field: rte_eth_rxseg_capa::reserved"]
        [::core::mem::offset_of!(rte_eth_rxseg_capa, reserved) - 6usize];
};
impl rte_eth_rxseg_capa {
    #[inline]
    pub fn multi_pools(&self) -> u32 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(0usize, 1u8) as u32) }
    }
    #[inline]
    pub fn set_multi_pools(&mut self, val: u32) {
        unsafe {
            let val: u32 = ::core::mem::transmute(val);
            self._bitfield_1.set(0usize, 1u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn multi_pools_raw(this: *const Self) -> u32 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 1usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                0usize,
                1u8,
            ) as u32)
        }
    }
    #[inline]
    pub unsafe fn set_multi_pools_raw(this: *mut Self, val: u32) {
        unsafe {
            let val: u32 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 1usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                0usize,
                1u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn offset_allowed(&self) -> u32 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(1usize, 1u8) as u32) }
    }
    #[inline]
    pub fn set_offset_allowed(&mut self, val: u32) {
        unsafe {
            let val: u32 = ::core::mem::transmute(val);
            self._bitfield_1.set(1usize, 1u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn offset_allowed_raw(this: *const Self) -> u32 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 1usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                1usize,
                1u8,
            ) as u32)
        }
    }
    #[inline]
    pub unsafe fn set_offset_allowed_raw(this: *mut Self, val: u32) {
        unsafe {
            let val: u32 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 1usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                1usize,
                1u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn offset_align_log2(&self) -> u32 {
        unsafe { ::core::mem::transmute(self._bitfield_1.get(2usize, 4u8) as u32) }
    }
    #[inline]
    pub fn set_offset_align_log2(&mut self, val: u32) {
        unsafe {
            let val: u32 = ::core::mem::transmute(val);
            self._bitfield_1.set(2usize, 4u8, val as u64)
        }
    }
    #[inline]
    pub unsafe fn offset_align_log2_raw(this: *const Self) -> u32 {
        unsafe {
            ::core::mem::transmute(<__BindgenBitfieldUnit<[u8; 1usize]>>::raw_get(
                ::core::ptr::addr_of!((*this)._bitfield_1),
                2usize,
                4u8,
            ) as u32)
        }
    }
    #[inline]
    pub unsafe fn set_offset_align_log2_raw(this: *mut Self, val: u32) {
        unsafe {
            let val: u32 = ::core::mem::transmute(val);
            <__BindgenBitfieldUnit<[u8; 1usize]>>::raw_set(
                ::core::ptr::addr_of_mut!((*this)._bitfield_1),
                2usize,
                4u8,
                val as u64,
            )
        }
    }
    #[inline]
    pub fn new_bitfield_1(
        multi_pools: u32,
        offset_allowed: u32,
        offset_align_log2: u32,
    ) -> __BindgenBitfieldUnit<[u8; 1usize]> {
        let mut __bindgen_bitfield_unit: __BindgenBitfieldUnit<[u8; 1usize]> = Default::default();
        __bindgen_bitfield_unit.set(0usize, 1u8, {
            let multi_pools: u32 = unsafe { ::core::mem::transmute(multi_pools) };
            multi_pools as u64
        });
        __bindgen_bitfield_unit.set(1usize, 1u8, {
            let offset_allowed: u32 = unsafe { ::core::mem::transmute(offset_allowed) };
            offset_allowed as u64
        });
        __bindgen_bitfield_unit.set(2usize, 4u8, {
            let offset_align_log2: u32 = unsafe { ::core::mem::transmute(offset_align_log2) };
            offset_align_log2 as u64
        });
        __bindgen_bitfield_unit
    }
}
#[doc = " No error handling modes are supported."]
pub const rte_eth_err_handle_mode_RTE_ETH_ERROR_HANDLE_MODE_NONE: rte_eth_err_handle_mode = 0;
#[doc = " Passive error handling, after the PMD detects that a reset is required,\n the PMD reports @see RTE_ETH_EVENT_INTR_RESET event,\n and the application invokes @see rte_eth_dev_reset to recover the port."]
pub const rte_eth_err_handle_mode_RTE_ETH_ERROR_HANDLE_MODE_PASSIVE: rte_eth_err_handle_mode = 1;
#[doc = " Proactive error handling, after the PMD detects that a reset is required,\n the PMD reports @see RTE_ETH_EVENT_ERR_RECOVERING event,\n do recovery internally, and finally reports the recovery result event\n (@see RTE_ETH_EVENT_RECOVERY_*)."]
pub const rte_eth_err_handle_mode_RTE_ETH_ERROR_HANDLE_MODE_PROACTIVE: rte_eth_err_handle_mode = 2;
#[doc = " @warning\n @b EXPERIMENTAL: this enumeration may change without prior notice.\n\n Ethernet device error handling mode."]
pub type rte_eth_err_handle_mode = ::core::ffi::c_int;
#[doc = " A structure used to retrieve the contextual information of\n an Ethernet device, such as the controlling driver of the\n device, etc..."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct rte_eth_dev_info {
    #[doc = "< Generic device information"]
    pub device: *mut rte_device,
    #[doc = "< Device Driver name."]
    pub driver_name: *const ::core::ffi::c_char,
    #[doc = "< Index to bound host interface, or 0 if none.\nUse if_indextoname() to translate into an interface name."]
    pub if_index: ::core::ffi::c_uint,
    #[doc = "< Minimum MTU allowed"]
    pub min_mtu: u16,
    #[doc = "< Maximum MTU allowed"]
    pub max_mtu: u16,
    #[doc = "< Device flags"]
    pub dev_flags: *const u32,
    #[doc = " Minimum Rx buffer size per descriptor supported by HW."]
    pub min_rx_bufsize: u32,
    #[doc = " Maximum Rx buffer size per descriptor supported by HW.\n The value is not enforced, information only to application to\n optimize mbuf size.\n Its value is UINT32_MAX when not specified by the driver."]
    pub max_rx_bufsize: u32,
    #[doc = "< Maximum configurable length of Rx pkt."]
    pub max_rx_pktlen: u32,
    #[doc = " Maximum configurable size of LRO aggregated packet."]
    pub max_lro_pkt_size: u32,
    #[doc = "< Maximum number of Rx queues."]
    pub max_rx_queues: u16,
    #[doc = "< Maximum number of Tx queues."]
    pub max_tx_queues: u16,
    #[doc = "< Maximum number of MAC addresses."]
    pub max_mac_addrs: u32,
    #[doc = " Maximum number of hash MAC addresses for MTA and UTA."]
    pub max_hash_mac_addrs: u32,
    #[doc = "< Maximum number of VFs."]
    pub max_vfs: u16,
    #[doc = "< Maximum number of VMDq pools."]
    pub max_vmdq_pools: u16,
    #[doc = "< Segmentation capability."]
    pub rx_seg_capa: rte_eth_rxseg_capa,
    #[doc = " All Rx offload capabilities including all per-queue ones"]
    pub rx_offload_capa: u64,
    #[doc = " All Tx offload capabilities including all per-queue ones"]
    pub tx_offload_capa: u64,
    #[doc = " Device per-queue Rx offload capabilities."]
    pub rx_queue_offload_capa: u64,
    #[doc = " Device per-queue Tx offload capabilities."]
    pub tx_queue_offload_capa: u64,
    #[doc = " Device redirection table size, the total number of entries."]
    pub reta_size: u16,
    #[doc = "< Hash key size in bytes"]
    pub hash_key_size: u8,
    pub rss_algo_capa: u32,
    #[doc = " RSS hash algorithms capabilities */\n/** Bit mask of RSS offloads, the bit offset also means flow type"]
    pub flow_type_rss_offloads: u64,
    #[doc = "< Default Rx configuration"]
    pub default_rxconf: rte_eth_rxconf,
    #[doc = "< Default Tx configuration"]
    pub default_txconf: rte_eth_txconf,
    #[doc = "< First queue ID for VMDq pools."]
    pub vmdq_queue_base: u16,
    #[doc = "< Queue number for VMDq pools."]
    pub vmdq_queue_num: u16,
    #[doc = "< First ID of VMDq pools."]
    pub vmdq_pool_base: u16,
    #[doc = "< Rx descriptors limits"]
    pub rx_desc_lim: rte_eth_desc_lim,
    #[doc = "< Tx descriptors limits"]
    pub tx_desc_lim: rte_eth_desc_lim,
    #[doc = "< Supported speeds bitmap (RTE_ETH_LINK_SPEED_)."]
    pub speed_capa: u32,
    #[doc = "< Number of Rx queues."]
    pub nb_rx_queues: u16,
    #[doc = "< Number of Tx queues."]
    pub nb_tx_queues: u16,
    #[doc = " Maximum number of Rx mempools supported per Rx queue.\n\n Value greater than 0 means that the driver supports Rx queue\n mempools specification via rx_conf->rx_mempools."]
    pub max_rx_mempools: u16,
    #[doc = " Rx parameter recommendations"]
    pub default_rxportconf: rte_eth_dev_portconf,
    #[doc = " Tx parameter recommendations"]
    pub default_txportconf: rte_eth_dev_portconf,
    #[doc = " Generic device capabilities (RTE_ETH_DEV_CAPA_)."]
    pub dev_capa: u64,
    #[doc = " Switching information for ports on a device with a\n embedded managed interconnect/switch."]
    pub switch_info: rte_eth_switch_info,
    #[doc = " Supported error handling mode."]
    pub err_handle_mode: rte_eth_err_handle_mode,
    #[doc = "< Reserved for future fields"]
    pub reserved_64s: [u64; 2usize],
    #[doc = "< Reserved for future fields"]
    pub reserved_ptrs: [*mut ::core::ffi::c_void; 2usize],
}
#[allow(clippy::unnecessary_operation, clippy::identity_op)]
const _: () = {
    ["Size of rte_eth_dev_info"][::core::mem::size_of::<rte_eth_dev_info>() - 376usize];
    ["Alignment of rte_eth_dev_info"][::core::mem::align_of::<rte_eth_dev_info>() - 8usize];
    ["Offset of field: rte_eth_dev_info::device"]
        [::core::mem::offset_of!(rte_eth_dev_info, device) - 0usize];
    ["Offset of field: rte_eth_dev_info::driver_name"]
        [::core::mem::offset_of!(rte_eth_dev_info, driver_name) - 8usize];
    ["Offset of field: rte_eth_dev_info::if_index"]
        [::core::mem::offset_of!(rte_eth_dev_info, if_index) - 16usize];
    ["Offset of field: rte_eth_dev_info::min_mtu"]
        [::core::mem::offset_of!(rte_eth_dev_info, min_mtu) - 20usize];
    ["Offset of field: rte_eth_dev_info::max_mtu"]
        [::core::mem::offset_of!(rte_eth_dev_info, max_mtu) - 22usize];
    ["Offset of field: rte_eth_dev_info::dev_flags"]
        [::core::mem::offset_of!(rte_eth_dev_info, dev_flags) - 24usize];
    ["Offset of field: rte_eth_dev_info::min_rx_bufsize"]
        [::core::mem::offset_of!(rte_eth_dev_info, min_rx_bufsize) - 32usize];
    ["Offset of field: rte_eth_dev_info::max_rx_bufsize"]
        [::core::mem::offset_of!(rte_eth_dev_info, max_rx_bufsize) - 36usize];
    ["Offset of field: rte_eth_dev_info::max_rx_pktlen"]
        [::core::mem::offset_of!(rte_eth_dev_info, max_rx_pktlen) - 40usize];
    ["Offset of field: rte_eth_dev_info::max_lro_pkt_size"]
        [::core::mem::offset_of!(rte_eth_dev_info, max_lro_pkt_size) - 44usize];
    ["Offset of field: rte_eth_dev_info::max_rx_queues"]
        [::core::mem::offset_of!(rte_eth_dev_info, max_rx_queues) - 48usize];
    ["Offset of field: rte_eth_dev_info::max_tx_queues"]
        [::core::mem::offset_of!(rte_eth_dev_info, max_tx_queues) - 50usize];
    ["Offset of field: rte_eth_dev_info::max_mac_addrs"]
        [::core::mem::offset_of!(rte_eth_dev_info, max_mac_addrs) - 52usize];
    ["Offset of field: rte_eth_dev_info::max_hash_mac_addrs"]
        [::core::mem::offset_of!(rte_eth_dev_info, max_hash_mac_addrs) - 56usize];
    ["Offset of field: rte_eth_dev_info::max_vfs"]
        [::core::mem::offset_of!(rte_eth_dev_info, max_vfs) - 60usize];
    ["Offset of field: rte_eth_dev_info::max_vmdq_pools"]
        [::core::mem::offset_of!(rte_eth_dev_info, max_vmdq_pools) - 62usize];
    ["Offset of field: rte_eth_dev_info::rx_seg_capa"]
        [::core::mem::offset_of!(rte_eth_dev_info, rx_seg_capa) - 64usize];
    ["Offset of field: rte_eth_dev_info::rx_offload_capa"]
        [::core::mem::offset_of!(rte_eth_dev_info, rx_offload_capa) - 72usize];
    ["Offset of field: rte_eth_dev_info::tx_offload_capa"]
        [::core::mem::offset_of!(rte_eth_dev_info, tx_offload_capa) - 80usize];
    ["Offset of field: rte_eth_dev_info::rx_queue_offload_capa"]
        [::core::mem::offset_of!(rte_eth_dev_info, rx_queue_offload_capa) - 88usize];
    ["Offset of field: rte_eth_dev_info::tx_queue_offload_capa"]
        [::core::mem::offset_of!(rte_eth_dev_info, tx_queue_offload_capa) - 96usize];
    ["Offset of field: rte_eth_dev_info::reta_size"]
        [::core::mem::offset_of!(rte_eth_dev_info, reta_size) - 104usize];
    ["Offset of field: rte_eth_dev_info::hash_key_size"]
        [::core::mem::offset_of!(rte_eth_dev_info, hash_key_size) - 106usize];
    ["Offset of field: rte_eth_dev_info::rss_algo_capa"]
        [::core::mem::offset_of!(rte_eth_dev_info, rss_algo_capa) - 108usize];
    ["Offset of field: rte_eth_dev_info::flow_type_rss_offloads"]
        [::core::mem::offset_of!(rte_eth_dev_info, flow_type_rss_offloads) - 112usize];
    ["Offset of field: rte_eth_dev_info::default_rxconf"]
        [::core::mem::offset_of!(rte_eth_dev_info, default_rxconf) - 120usize];
    ["Offset of field: rte_eth_dev_info::default_txconf"]
        [::core::mem::offset_of!(rte_eth_dev_info, default_txconf) - 200usize];
    ["Offset of field: rte_eth_dev_info::vmdq_queue_base"]
        [::core::mem::offset_of!(rte_eth_dev_info, vmdq_queue_base) - 256usize];
    ["Offset of field: rte_eth_dev_info::vmdq_queue_num"]
        [::core::mem::offset_of!(rte_eth_dev_info, vmdq_queue_num) - 258usize];
    ["Offset of field: rte_eth_dev_info::vmdq_pool_base"]
        [::core::mem::offset_of!(rte_eth_dev_info, vmdq_pool_base) - 260usize];
    ["Offset of field: rte_eth_dev_info::rx_desc_lim"]
        [::core::mem::offset_of!(rte_eth_dev_info, rx_desc_lim) - 262usize];
    ["Offset of field: rte_eth_dev_info::tx_desc_lim"]
        [::core::mem::offset_of!(rte_eth_dev_info, tx_desc_lim) - 272usize];
    ["Offset of field: rte_eth_dev_info::speed_capa"]
        [::core::mem::offset_of!(rte_eth_dev_info, speed_capa) - 284usize];
    ["Offset of field: rte_eth_dev_info::nb_rx_queues"]
        [::core::mem::offset_of!(rte_eth_dev_info, nb_rx_queues) - 288usize];
    ["Offset of field: rte_eth_dev_info::nb_tx_queues"]
        [::core::mem::offset_of!(rte_eth_dev_info, nb_tx_queues) - 290usize];
    ["Offset of field: rte_eth_dev_info::max_rx_mempools"]
        [::core::mem::offset_of!(rte_eth_dev_info, max_rx_mempools) - 292usize];
    ["Offset of field: rte_eth_dev_info::default_rxportconf"]
        [::core::mem::offset_of!(rte_eth_dev_info, default_rxportconf) - 294usize];
    ["Offset of field: rte_eth_dev_info::default_txportconf"]
        [::core::mem::offset_of!(rte_eth_dev_info, default_txportconf) - 300usize];
    ["Offset of field: rte_eth_dev_info::dev_capa"]
        [::core::mem::offset_of!(rte_eth_dev_info, dev_capa) - 312usize];
    ["Offset of field: rte_eth_dev_info::switch_info"]
        [::core::mem::offset_of!(rte_eth_dev_info, switch_info) - 320usize];
    ["Offset of field: rte_eth_dev_info::err_handle_mode"]
        [::core::mem::offset_of!(rte_eth_dev_info, err_handle_mode) - 336usize];
    ["Offset of field: rte_eth_dev_info::reserved_64s"]
        [::core::mem::offset_of!(rte_eth_dev_info, reserved_64s) - 344usize];
    ["Offset of field: rte_eth_dev_info::reserved_ptrs"]
        [::core::mem::offset_of!(rte_eth_dev_info, reserved_ptrs) - 360usize];
};
unsafe extern "C" {
    #[doc = " Get the number of ports which are usable for the application.\n\n These devices must be iterated by using the macro\n ``RTE_ETH_FOREACH_DEV`` or ``RTE_ETH_FOREACH_DEV_OWNED_BY``\n to deal with non-contiguous ranges of devices.\n\n @return\n   The count of available Ethernet devices."]
    pub fn rte_eth_dev_count_avail() -> u16;
}
unsafe extern "C" {
    #[doc = " Configure an Ethernet device.\n This function must be invoked first before any other function in the\n Ethernet API. This function can also be re-invoked when a device is in the\n stopped state.\n\n @param port_id\n   The port identifier of the Ethernet device to configure.\n @param nb_rx_queue\n   The number of receive queues to set up for the Ethernet device.\n @param nb_tx_queue\n   The number of transmit queues to set up for the Ethernet device.\n @param eth_conf\n   The pointer to the configuration data to be used for the Ethernet device.\n   The *rte_eth_conf* structure includes:\n     -  the hardware offload features to activate, with dedicated fields for\n        each statically configurable offload hardware feature provided by\n        Ethernet devices, such as IP checksum or VLAN tag stripping for\n        example.\n        The Rx offload bitfield API is obsolete and will be deprecated.\n        Applications should set the ignore_bitfield_offloads bit on *rxmode*\n        structure and use offloads field to set per-port offloads instead.\n     -  Any offloading set in eth_conf->[rt]xmode.offloads must be within\n        the [rt]x_offload_capa returned from rte_eth_dev_info_get().\n        Any type of device supported offloading set in the input argument\n        eth_conf->[rt]xmode.offloads to rte_eth_dev_configure() is enabled\n        on all queues and it can't be disabled in rte_eth_[rt]x_queue_setup()\n     -  the Receive Side Scaling (RSS) configuration when using multiple Rx\n        queues per port. Any RSS hash function set in eth_conf->rss_conf.rss_hf\n        must be within the flow_type_rss_offloads provided by drivers via\n        rte_eth_dev_info_get() API.\n\n   Embedding all configuration information in a single data structure\n   is the more flexible method that allows the addition of new features\n   without changing the syntax of the API.\n @return\n   - 0: Success, device configured.\n   - <0: Error code returned by the driver configuration function."]
    pub fn rte_eth_dev_configure(
        port_id: u16,
        nb_rx_queue: u16,
        nb_tx_queue: u16,
        eth_conf: *const rte_eth_conf,
    ) -> ::core::ffi::c_int;
}
unsafe extern "C" {
    #[doc = " Allocate and set up a receive queue for an Ethernet device.\n\n The function allocates a contiguous block of memory for *nb_rx_desc*\n receive descriptors from a memory zone associated with *socket_id*\n and initializes each receive descriptor with a network buffer allocated\n from the memory pool *mb_pool*.\n\n @param port_id\n   The port identifier of the Ethernet device.\n @param rx_queue_id\n   The index of the receive queue to set up.\n   The value must be in the range [0, nb_rx_queue - 1] previously supplied\n   to rte_eth_dev_configure().\n @param nb_rx_desc\n   The number of receive descriptors to allocate for the receive ring.\n @param socket_id\n   The *socket_id* argument is the socket identifier in case of NUMA.\n   The value can be *SOCKET_ID_ANY* if there is no NUMA constraint for\n   the DMA memory allocated for the receive descriptors of the ring.\n @param rx_conf\n   The pointer to the configuration data to be used for the receive queue.\n   NULL value is allowed, in which case default Rx configuration\n   will be used.\n   The *rx_conf* structure contains an *rx_thresh* structure with the values\n   of the Prefetch, Host, and Write-Back threshold registers of the receive\n   ring.\n   In addition it contains the hardware offloads features to activate using\n   the RTE_ETH_RX_OFFLOAD_* flags.\n   If an offloading set in rx_conf->offloads\n   hasn't been set in the input argument eth_conf->rxmode.offloads\n   to rte_eth_dev_configure(), it is a new added offloading, it must be\n   per-queue type and it is enabled for the queue.\n   No need to repeat any bit in rx_conf->offloads which has already been\n   enabled in rte_eth_dev_configure() at port level. An offloading enabled\n   at port level can't be disabled at queue level.\n   The configuration structure also contains the pointer to the array\n   of the receiving buffer segment descriptions, see rx_seg and rx_nseg\n   fields, this extended configuration might be used by split offloads like\n   RTE_ETH_RX_OFFLOAD_BUFFER_SPLIT. If mb_pool is not NULL,\n   the extended configuration fields must be set to NULL and zero.\n @param mb_pool\n   The pointer to the memory pool from which to allocate *rte_mbuf* network\n   memory buffers to populate each descriptor of the receive ring. There are\n   two options to provide Rx buffer configuration:\n   - single pool:\n     mb_pool is not NULL, rx_conf.rx_nseg is 0.\n   - multiple segments description:\n     mb_pool is NULL, rx_conf.rx_seg is not NULL, rx_conf.rx_nseg is not 0.\n     Taken only if flag RTE_ETH_RX_OFFLOAD_BUFFER_SPLIT is set in offloads.\n\n @return\n   - 0: Success, receive queue correctly set up.\n   - -EIO: if device is removed.\n   - -ENODEV: if *port_id* is invalid.\n   - -EINVAL: The memory pool pointer is null or the size of network buffers\n      which can be allocated from this memory pool does not fit the various\n      buffer sizes allowed by the device controller.\n   - -ENOMEM: Unable to allocate the receive ring descriptors or to\n      allocate network memory buffers from the memory pool when\n      initializing receive descriptors."]
    pub fn rte_eth_rx_queue_setup(
        port_id: u16,
        rx_queue_id: u16,
        nb_rx_desc: u16,
        socket_id: ::core::ffi::c_uint,
        rx_conf: *const rte_eth_rxconf,
        mb_pool: *mut rte_mempool,
    ) -> ::core::ffi::c_int;
}
unsafe extern "C" {
    #[doc = " Allocate and set up a transmit queue for an Ethernet device.\n\n @param port_id\n   The port identifier of the Ethernet device.\n @param tx_queue_id\n   The index of the transmit queue to set up.\n   The value must be in the range [0, nb_tx_queue - 1] previously supplied\n   to rte_eth_dev_configure().\n @param nb_tx_desc\n   The number of transmit descriptors to allocate for the transmit ring.\n @param socket_id\n   The *socket_id* argument is the socket identifier in case of NUMA.\n   Its value can be *SOCKET_ID_ANY* if there is no NUMA constraint for\n   the DMA memory allocated for the transmit descriptors of the ring.\n @param tx_conf\n   The pointer to the configuration data to be used for the transmit queue.\n   NULL value is allowed, in which case default Tx configuration\n   will be used.\n   The *tx_conf* structure contains the following data:\n   - The *tx_thresh* structure with the values of the Prefetch, Host, and\n     Write-Back threshold registers of the transmit ring.\n     When setting Write-Back threshold to the value greater then zero,\n     *tx_rs_thresh* value should be explicitly set to one.\n   - The *tx_free_thresh* value indicates the [minimum] number of network\n     buffers that must be pending in the transmit ring to trigger their\n     [implicit] freeing by the driver transmit function.\n   - The *tx_rs_thresh* value indicates the [minimum] number of transmit\n     descriptors that must be pending in the transmit ring before setting the\n     RS bit on a descriptor by the driver transmit function.\n     The *tx_rs_thresh* value should be less or equal then\n     *tx_free_thresh* value, and both of them should be less then\n     *nb_tx_desc* - 3.\n   - The *offloads* member contains Tx offloads to be enabled.\n     If an offloading set in tx_conf->offloads\n     hasn't been set in the input argument eth_conf->txmode.offloads\n     to rte_eth_dev_configure(), it is a new added offloading, it must be\n     per-queue type and it is enabled for the queue.\n     No need to repeat any bit in tx_conf->offloads which has already been\n     enabled in rte_eth_dev_configure() at port level. An offloading enabled\n     at port level can't be disabled at queue level.\n\n     Note that setting *tx_free_thresh* or *tx_rs_thresh* value to 0 forces\n     the transmit function to use default values.\n @return\n   - 0: Success, the transmit queue is correctly set up.\n   - -ENOMEM: Unable to allocate the transmit ring descriptors."]
    pub fn rte_eth_tx_queue_setup(
        port_id: u16,
        tx_queue_id: u16,
        nb_tx_desc: u16,
        socket_id: ::core::ffi::c_uint,
        tx_conf: *const rte_eth_txconf,
    ) -> ::core::ffi::c_int;
}
unsafe extern "C" {
    #[doc = " Return the NUMA socket to which an Ethernet device is connected\n\n @param port_id\n   The port identifier of the Ethernet device\n @return\n   - The NUMA socket ID which the Ethernet device is connected to.\n   - -1 (which translates to SOCKET_ID_ANY) if the socket could not be\n     determined. rte_errno is then set to:\n     - EINVAL is the port_id is invalid,\n     - 0 is the socket could not be determined,"]
    pub fn rte_eth_dev_socket_id(port_id: u16) -> ::core::ffi::c_int;
}
unsafe extern "C" {
    #[doc = " Start an Ethernet device.\n\n The device start step is the last one and consists of setting the configured\n offload features and in starting the transmit and the receive units of the\n device.\n\n Device RTE_ETH_DEV_NOLIVE_MAC_ADDR flag causes MAC address to be set before\n PMD port start callback function is invoked.\n\n All device queues (except form deferred start queues) status should be\n `RTE_ETH_QUEUE_STATE_STARTED` after start.\n\n On success, all basic functions exported by the Ethernet API (link status,\n receive/transmit, and so on) can be invoked.\n\n @param port_id\n   The port identifier of the Ethernet device.\n @return\n   - 0: Success, Ethernet device started.\n   - -EAGAIN: If start operation must be retried.\n   - <0: Error code of the driver device start function."]
    pub fn rte_eth_dev_start(port_id: u16) -> ::core::ffi::c_int;
}
unsafe extern "C" {
    #[doc = " Stop an Ethernet device. The device can be restarted with a call to\n rte_eth_dev_start()\n\n All device queues status should be `RTE_ETH_QUEUE_STATE_STOPPED` after stop.\n\n @param port_id\n   The port identifier of the Ethernet device.\n @return\n   - 0: Success, Ethernet device stopped.\n   - -EBUSY: If stopping the port is not allowed in current state.\n   - <0: Error code of the driver device stop function."]
    pub fn rte_eth_dev_stop(port_id: u16) -> ::core::ffi::c_int;
}
unsafe extern "C" {
    #[doc = " Close a stopped Ethernet device. The device cannot be restarted!\n The function frees all port resources.\n\n @param port_id\n   The port identifier of the Ethernet device.\n @return\n   - Zero if the port is closed successfully.\n   - Negative if something went wrong."]
    pub fn rte_eth_dev_close(port_id: u16) -> ::core::ffi::c_int;
}
unsafe extern "C" {
    #[doc = " Enable receipt in promiscuous mode for an Ethernet device.\n\n @param port_id\n   The port identifier of the Ethernet device.\n @return\n   - (0) if successful.\n   - (-ENOTSUP) if support for promiscuous_enable() does not exist\n     for the device.\n   - (-ENODEV) if *port_id* invalid."]
    pub fn rte_eth_promiscuous_enable(port_id: u16) -> ::core::ffi::c_int;
}
unsafe extern "C" {
    pub fn rte_eth_allmulticast_enable(port_id: u16) -> ::core::ffi::c_int;
}
unsafe extern "C" {
    #[doc = " Retrieve the Ethernet address of an Ethernet device.\n\n @param port_id\n   The port identifier of the Ethernet device.\n @param mac_addr\n   A pointer to a structure of type *ether_addr* to be filled with\n   the Ethernet address of the Ethernet device.\n @return\n   - (0) if successful\n   - (-ENODEV) if *port_id* invalid.\n   - (-EINVAL) if bad parameter."]
    pub fn rte_eth_macaddr_get(port_id: u16, mac_addr: *mut rte_ether_addr) -> ::core::ffi::c_int;
}
unsafe extern "C" {
    #[doc = " Retrieve the contextual information of an Ethernet device.\n\n This function returns the Ethernet device information based\n on the values stored internally in the device specific data.\n For example: number of queues, descriptor limits, device\n capabilities and offload flags.\n\n @param port_id\n   The port identifier of the Ethernet device.\n @param dev_info\n   A pointer to a structure of type *rte_eth_dev_info* to be filled with\n   the contextual information of the Ethernet device.\n @return\n   - (0) if successful.\n   - (-ENOTSUP) if support for dev_infos_get() does not exist for the device.\n   - (-ENODEV) if *port_id* invalid.\n   - (-EINVAL) if bad parameter."]
    pub fn rte_eth_dev_info_get(
        port_id: u16,
        dev_info: *mut rte_eth_dev_info,
    ) -> ::core::ffi::c_int;
}

// ============================================================================
// Wrapper functions for DPDK inline functions
// These must be provided by dpdk_wrappers.c compiled as a static library
// ============================================================================

unsafe extern "C" {
    pub fn dpdk_wrap_rte_eth_rx_burst(
        port_id: u16,
        queue_id: u16,
        rx_pkts: *mut *mut rte_mbuf,
        nb_pkts: u16,
    ) -> u16;

    pub fn dpdk_wrap_rte_eth_tx_burst(
        port_id: u16,
        queue_id: u16,
        tx_pkts: *mut *mut rte_mbuf,
        nb_pkts: u16,
    ) -> u16;

    pub fn dpdk_wrap_rte_pktmbuf_alloc(mp: *mut rte_mempool) -> *mut rte_mbuf;

    pub fn dpdk_wrap_rte_pktmbuf_free(m: *mut rte_mbuf);

    pub fn dpdk_wrap_rte_pktmbuf_mtod(m: *mut rte_mbuf) -> *mut ::core::ffi::c_void;

    pub fn dpdk_wrap_rte_pktmbuf_data_len(m: *const rte_mbuf) -> u16;

    pub fn dpdk_wrap_rte_pktmbuf_pkt_len(m: *const rte_mbuf) -> u32;

    pub fn dpdk_wrap_rte_pktmbuf_append(m: *mut rte_mbuf, len: u16) -> *mut ::core::ffi::c_char;

    pub fn dpdk_wrap_rte_pktmbuf_headroom(m: *const rte_mbuf) -> u16;

    pub fn dpdk_wrap_rte_pktmbuf_tailroom(m: *const rte_mbuf) -> u16;
}
